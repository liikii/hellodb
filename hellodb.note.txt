three main types of failures:
» Disk (storage media) failure
» System crash
» Transaction failure


princeton cs 145 intro to dbs.

Database Management Systems, 3rd Edition
by Raghu Ramakrishnan and Johannes Gehrke
The following book is required for the class. A few lectures will review material from this textbook and will include readings from this book. Please make sure to get the third edition of this book.

Database Management Systems, 3rd Edition. Raghu Ramakrishnan and Johannes Gehrke.
The following are additional books that you may find useful:

Foundations of Databases (Abiteboul, Hull & Vianu)
Database Systems: the complete book (Ullman, Widom and Garcia-Molina)
Parallel and Distributed DBMS (Ozsu and Valduriez)
Transaction Processing (Gray and Reuter)
Data and Knowledge based Systems (volumes I, II) (Ullman)
Data on the Web (Abiteboul, Buneman, Suciu)
Readings in Database Systems (4th and 3rd ed) (Stonebraker and Hellerstein)
Proceedings of SIGMOD, VLDB, ICDE, and CIDR conferences.

### http://www.scs.stanford.edu/20sp-cs244b/notes/  distributed system
### http://www.scs.stanford.edu/20sp-cs244b/notes/
## cs 149  PARALLEL COMPUTING 
## cs 149  PARALLEL COMPUTING
## cs 149  PARALLEL COMPUTING

Isolation levels:
» Level 1: Transaction may read uncommitted data; 
successive reads to a record may return different values
» Level 2: Transaction may only read committed data, but 
successive reads can differ
» Level 3: Successive reads return same value

Locking:
» Started with “predicate locks” based on expressions: 
too expensive
» Moved to hierarchical locks: record/page/table, with 
read/write types and intentions



Transactional DBMS: focus on concurrent, 
small, low-latency transactions (e.g. MySQL, 
Postgres, Oracle, DB2) → real-time apps
Analytical DBMS: focus on large, parallel 
but mostly read-only analytics (e.g. Teradata, 
Redshift, Vertica) → “data warehouses


Fixed Format
A schema for all records in table specifies:
- # of fields
- type of each field
- order in record
- meaning of each field


Placing Data for Efficient 
Access
Locality: which items are accessed together
» When you read one field of a record, you’re 
likely to read other fields of the same record
» When you read one field of record 1, you’re 
likely to read the same field of record 2
Searchability: quickly find relevant records
» E.g. sorting the file lets you do binary search



Can We Have Hybrids 
Between Row & Column?
Yes! For example, colocated column groups: 


Improving Searchability: 
Ordering
Ordering the data by a field will give:
» Closer I/Os if queries tend to read data with 
nearby values of the field (e.g. time ranges)
» Option to accelerate search via an ordered 
index (e.g. B-tree), binary search, etc


Questions in Storing Records
(1) separating records
(2) spanned vs. unspanned
(3) indirection


C-Store


Handling Duplicate Keys
For a primary index, can point to 1st instance 
of each item (assuming blocks are linked)
For a secondary index, need to point to a list 
of records since they can be anywhere


k-d tree m dimension index
Splits dimensions in any order 
to hold k-dimensional data


Wide range of indexes for different data 
types and queries (e.g. range vs exact)
Key concerns: query time, cost to update, 
and size of index


Execution Methods: Once We 
Have a Plan, How to Run it?
Several options that trade between 
complexity, performance and startup time


Outline
What can we optimize?
Rule-based optimization
Data statistics
Cost models
Cost-based plan selection


Join algorithms can have different 
performance in different situations
In general, the following are used:
» Index join if an index exists
» Merge join if at least one table is sorted
» Hash join if both tables unsorted


SQL standard defines serializability as “same 
as a serial schedule”, but then also lists 3 
types of “anomalies” to define levels:


There are isolation levels other than 
serializability that meet the last definition!
» I.e. don’t exhibit those 3 anomalies
Virtually no commercial DBs do serializability 
by default, and some can’t do it at all
Time to call the lawyers?


Enforcing serializability via 2-phase locking
» Shared and exclusive locks
» Lock tables and multi-level locking


Want schedules that are “good”, regardless of
» initial state and
» transaction semantics


------------------------------
Transactions
Concurrency
Transactions
Concurrency
Transactions
Concurrency
------------------------------

the eight fallacies of distributed computing
the eight fallacies of distributed computing
the eight fallacies of distributed computing


Replication
Store each data item on multiple nodes!



Traditional DB: page the DBA
Distributed computing: use consensus
» Several algorithms: Paxos, Raft
» Today: many implementations
• Apache Zookeeper, etcd, Consul
» Idea: keep a reliable, distributed shared 
record of who is “primary”


Two Phase Commit (2PC)
1. Transaction coordinator sends prepare
message to each participating node
2. Each participating node responds to 
coordinator with prepared or no
3. If coordinator receives all prepared:
» Broadcast commit
4. If coordinator receives any no:
» Broadcast abort


Traditionally: run 2PC at commit time
» i.e., perform locking as usual, then run 2PC 
to have all participants agree that the 
transaction will commit
Under strict 2PL, run 2PC before unlocking 
the write locks

Two-phase locking

2PL: piggyback lock “unlock” commands on 
commit/abort message

Case 2: Coordinator 
Unavailable
Participants cannot make progress
But: can agree to elect a new coordinator, 
never listen to the old one (using consensus)
» Old coordinator comes back? Overruled by 
participants, who reject its messages


CAP Theorem
In an asynchronous network, a distributed 
database can either:
» guarantee a response from any replica in a 
finite amount of time (“availability”) OR
» guarantee arbitrary “consistency” 
criteria/constraints about data
but not both


“CAP” is a reminder:
» No free lunch for distributed systems


CAP Theorem
Choose either:
» Consistency and “Partition tolerance” (CP)
» Availability and “Partition tolerance” (AP)
Example consistency criteria:
» Exactly one key can have value “Matei”
CAP is a reminder: no free lunch for 
distributed systems


Let’s Talk About Coordination
If we’re “AP”, then we don’t have to talk 
even when we can!
If we’re “CP”, then we have to talk all the 
time
How fast can we send messages


Punchlines:
Serializability has a provable cost to latency, 
availability, scalability (if there are conflicts)
We can avoid this penalty if we are willing to 
look at our application and our application 
does not require coordination
» Major topic of ongoing research



coordination avoidance in database systems
coordination avoidance in database systems


lessons from internet services: ACID vs. BASE
lessons from internet services: ACID vs. BASE


Avoiding Coordination
Key techniques for BASE:
» Partition data so that most transactions are 
local to one partition
» Tolerate stale data (eventual consistency):
• Caches
• Weaker isolation levels
• Helpful ideas: idempotence, commutativity



BASE EXAMPLE 
Constraint: each 
user’s amt_sold and 
amt_bought is sum of 
their transactions
ACID Approach: to add a transaction, use 2PC to 
update transactions table + records for buyer, seller
One BASE approach: write new transactions to the 
transactions table and use a periodic batch job to fill 
in the users table


Helpful Ideas
When we delay applying updates to an item, 
must ensure we only apply each update once
» Issue if we crash while applying!
» Idempotent operations: same result if you 
apply them twice
When different nodes want to update multiple 
items, want result independent of msg order
» Commutative operations: A⍟B = B⍟A


Amdahl’s Law
If p is the fraction of the program that can be 
made parallel, running time with N nodes is
T(n) = 1 - p + p/N
Result: max possible speedup is 1 / (1 - p)
Example: 80% parallelizable ⇒ 5x speedup


Example System Designs
Traditional “massively parallel” DBMS
» Tables partitioned evenly across nodes
» Each physical operator also partitioned
» Pipelining across these operators
MapReduce
» Focus on unreliable, commodity nodes
» Divide work into idempotent tasks, and use 
dynamic algorithms for load balancing, fault 
recovery and straggler recovery


Example: Distributed Joins
Say we want to compute A ⨝ B, where A 
and B are both partitioned across N nodes:


Example: Distributed Joins
Say we want to compute A ⨝ B, where A 
and B are both partitioned across N nodes
Algorithm 1: shuffle hash join
» Each node hashes records of A, B to N 
partitions by key, sends partition i to node i
» Each node then joins the records it received
Communication cost: (N-1)/N (|A| + |B|)


Algorithm 2: broadcast join on B
» Each node broadcasts its partition of B to all 
other nodes
» Each node then joins B against its A partition

Takeaway
Broadcast join is much faster if |B| ≪ |A|


Handling Faults & Stragglers
If uncommon, just ignore / call the operator / 
restart query
Problem: probability of something bad 
grows fast with number of nodes
» E.g. if one node has 0.1% probability of 
straggling, then with 1000 nodes,
P(none straggles) = (1 - 0.001)1000 ≈ 0.3


Straggler Recovery Methods
General idea: send the slow request/task to 
another node (launch a “backup task”)
Threshold approach: if a task is slower than 
99th percentile, or 1.5x avg, etc, launch backup
Progress-based approach:
estimate task finish times and launch
tasks likeliest to finish last


What is Cloud Computing?
Computing as a service, managed by an 
external party
» Software as a Service (SaaS): application 
hosted by a provider, e.g. Salesforce, Gmail
» Platform as a Service (PaaS): APIs to 
program against, e.g. DB or web hosting
» Infrastructure as a Service (IaaS): raw 
computing resources, e.g. VMs on AW


Dynamo Implementation
Commodity nodes with local 
storage on disks
Nodes form a “ring” to split up 
the key space among them
» Actually, each node covers 
many ranges (over-partitioning)
Use quorums and gossip to 
manage updates to each key


Just run an existing DBMS (e.g. MySQL) on 
cloud VMs, and use replicated disk storage


Aurora’s Design
Implement replication at a higher level: only 
replicate the redo log (not disk blocks)
Enable elastic frontend and backend by 
decoupling API & storage servers
» Lower cost & higher performance per tenant


Design Details
Logging uses async quorum: wait until 4 of 6 
nodes reply (faster than waiting for all 6)
Each storage node takes
the log and rebuilds the
DB pages locally
Care taken to handle
incomplete logs due
to async quorums

Delta Lake Motivation


Delta Lake Motivation
Object stores are the largest & lowest-cost 
storage systems, but their semantics make it 
hard to manage mutable datasets
Goal: analytical table storage over object 
stores, built as a client library (no other services)
Interface: relational tables with SQL queries
Consistency: serializable ACID transactions


CQL = Continuous Query Language; 
research project by our dean Jennifer 
Widom


More recent API, used at Google and open 
sourced (API only) as Apache Beam


Heap File Implementation 


定长记录， 插入中间， 得移动顺序？


Clustered/Unclustered 
• Primary index = clustered by definition 
• Secondary indexes = usually unclustered 


R tree index空间划分树

Canonical database search tree 
• Balanced tree with high fanout
• Leaf nodes contain pointers to actual data 
• Leaf nodes stored as a linked list 
• Internal nodes used as a directory 
– Contain <key,pointers> pairs 
– If key consistent with query, data may be found if we follow pointer 
– Generalized search key: predicate that holds for each entry below key 
• B+-tree key is pair of integers <a,b> and predicate is Contains([a,b),v) 
• R-tree key is bounding box and predicate is also containment test 
– Generalized search tree: hierarchy of partitions 



• Union
– Once a key is inserted, need to adjust predicates at parent nodes 
– See algorithm AdjustKeys(R,N) 
– B+-tree: computes interval that covers all given intervals 
– R-tree: computes bigger bounding box 


• Step 3: Query optimization
– Find an efficient query plan for executing the query 
– We will spend a whole lecture on this topic 
• A query plan is
– Logical query plan: an extended relational algebra tree 
– Physical query plan: with additional annotations at each node 
• Access method to use for each relation 
• Implementation to use for each relational operator 


Extended Algebra Operators 
• Union ∪, intersection ∩, difference - 
• Selection σ
• Projection π
• Join 
• Duplicate elimination δ
• Grouping and aggregation γ
• Sorting τ
• Rename ρ


• Most optimizers operate on individual query blocks 
• A query block is an SQL query with no nesting 
– Exactly one
• SELECT clause 
• FROM clause
– At most one
• WHERE clause 
• GROUP BY clause 
• HAVING clause


• Logical query plan with extra annotations 
• Access path selection for each relation 
– Use a file scan or use an index 
• Implementation choice for each operator 
• Scheduling decisions for operators


Why Learn About Op Algos? 
• Implemented in commercial DBMSs
• Different DBMSs implement different subsets of these 
algorithms 
• Good algorithms can greatly improve performance 
• Need to know about physical operators to understand 
query optimization


Cost Parameters 
• In database systems the data is on disk 
• Cost = total number of I/Os
• Parameters: 
– B(R) = # of blocks (i.e., pages) for relation R 
– T(R) = # of tuples in relation R 
– V(R, a) = # of distinct values of attribute a 


 Cost of an operation = number of disk I/Os to 
– read the operands 
– compute the result 
• Cost of writing the result to disk is not included
– Need to count it separately when applicable

• Clustered relation R: 
– Blocks consists mostly of records from this table 
– B(R) ≈ T(R) / blockSize 
• Unclustered relation R: 
– Its records are placed on blocks with other tables 
– When R is unclustered: B(R) ≈ T(R) 


Clustered relation: 
– Result may be unsorted: B(R) 
– Result needs to be sorted: 3B(R) 
• Unclustered relation 
– Unsorted: T(R) 
– Sorted: T(R) + 2B(R)


Join Algorithms 
• Logical operator: 
– Product(pname, cname) ⋈ Company(cname, city) 
• Propose three physical operators for the join, assuming 
the tables are in main memory: 
– Hash join 
– Nested loop join
– Sort-merge join


Hash Join 
Hash join: R ⋈ S 
• Scan R, build buckets in main memory 
• Then scan S and join 
• Cost: B(R) + B(S) 
• One pass algorithm when B(R) <= M （memory?）


Nested Loop Joins 
• Tuple-based nested loop R ⋈ S 
• R is the outer relation, S is the inner relation 
• Cost: B(R) + T(R) B(S) when S is clustered 
• Cost: B(R) + T(R) T(S) when S is unclustered 
for each tuple r in R do 
 for each tuple s in S do 
	 if r and s join then output (r,s) 


• Steps involved in processing a query 
– Logical query plan 
– Physical query plan 
– Query execution overview 
• Operator implementations 
– One pass algorithms 
– Two-pass algorithms 
– Index-based algorithms 


Two-Pass Algorithms 
• What if data does not fit in memory? 
• Need to process it in multiple passes 
• Two key techniques 
– Hashing 
– Sorting


Query Optimization Algorithm
• For a query 
– There exists many physical query plans 
– Query optimizer needs to pick a good one 
• Basic query optimization algorithm 
– Enumerate alternative plans 
– Compute estimated cost of each plan
• Compute number of I/Os 
• Optionally take into account other resources 
– Choose plan with lowest cost 
– This is called cost-based optimization



Access path selectivity is the number of pages 
retrieved if we use this access path
– Most selective retrieves fewest pages 
• As we saw earlier, for equality predicates
– Selection on equality: σa=v(R) 
– V(R, a) = # of distinct values of attribute a 
– 1/V(R,a) is thus the reduction factor 
– Clustered index on a: cost B(R)/V(R,a) 
– Unclustered index on a: cost T(R)/V(R,a) 
– (we are ignoring I/O cost of index pages for simplicity)



Selection on range: σa>v(R) 
• How to compute the selectivity?
• Assume values are uniformly distributed
• Reduction factor X 
• X = (Max(R,a) - v) / (Max(R,a) - Min(R,a)) 
• Clustered index on a: cost B(R)*X 
• Unclustered index on a: cost T(R)*X


• Selection condition: sid > 300 ∧ scity=‘Seattle’ 
– Index I1: B+-tree on sid clustered 
– Index I2: B+-tree on scity unclustered 
• Let’s assume 
– V(Supplier,scity) = 20 
– Max(Supplier, sid) = 1000, Min(Supplier,sid)=1 
– B(Supplier) = 100, T(Supplier) = 1000 
• Cost I1: B(R) * (Max-v)/(Max-Min) = 100*700/999 ≈ 70 
• Cost I2: T(R) * 1/V(Supplier,scity) = 1000/20 = 50 


• For each operator compute
– Estimate cost of executing the operation
– Estimate statistical summary of the output data


Relational Algebra Equivalences 
• Selects, projects, and joins 
– We can commute and combine all three types of operators 
– We just have to be careful that the fields we need are available 
when we apply the operator
– Relatively straightforward. See book 15.3. 
• If you like this topic, more info in optional paper (by 
Chaudhuri), Section 4. 


Plan Enumeration Algorithm 
• Idea: use dynamic programming
• For each subset of {R1, …, Rn}, compute the best plan 
for that subset 
• In increasing order of set cardinality: 
– Step 1: for {R1}, {R2}, …, {Rn} 
– Step 2: for {R1,R2}, {R1,R3}, …, {Rn-1, Rn} 
– … 
– Step n: for {R1, …, Rn} 
• It is a bottom-up strategy 
• A subset of {R1, …, Rn} is also called a subquery 


 Turing awards to database researchers: 
– Charles Bachman 1973 
– Edgar Codd 1981 for inventing relational dbs
– Jim Gray 1998 for inventing transactions 


ACID Properties 
• Atomicity: Either all changes performed by transaction 
occur or none occurs 
• Consistency: A transaction as a whole does not violate 
integrity constraints 
• Isolation: Transactions appear to execute one after the 
other in sequence 
• Durability: If a transaction commits, its changes will survive 
failures


Why is it hard to provide ACID properties? 
• Concurrent operations 
– Isolation problems 
– We saw one example earlier 
• Failures can occur at any time 
– Atomicity and durability problems 
– Next lecture 
• Transaction may need to abort


Types of Problems: Summary 
• Concurrent execution problems
– Write-read conflict: dirty read
• A transaction reads a value written by another transaction that has 
not yet committed 
– Read-write conflict: unrepeatable read
• A transaction reads the value of the same object twice. Another 
transaction modifies that value in between the two reads 
– Write-write conflict: lost update
• Two transactions update the value of the same object. The second 
one to write the value overwrite the first change 



Deadlocks 
• Two or more transactions are waiting for each other to 
complete 
• Deadlock avoidance
– Acquire locks in pre-defined order 
– Acquire all locks at once before starting 
• Deadlock detection
– Timeouts 
– Wait-for graph 
• This is what commercial systems use (they check graph periodically


Degrees of Isolation 
• Isolation level “serializable” (i.e. ACID) 
– Golden standard 
– Requires strict 2PL and predicate locking 
– But often too inefficient 
– Imagine there are only a few update operations and many long 
read operations 
• Weaker isolation levels 
– Sacrifice correctness for efficiency 
– Often used in practice (often default) 
– Sometimes are hard to understand 


Four levels of isolation
– All levels use long-duration exclusive locks
– READ UNCOMMITTED: no read locks 
– READ COMMITTED: short duration read locks 
– REPEATABLE READ: 
• Long duration read locks on individual items 
– SERIALIZABLE: 
• All locks long duration and lock predicates


Lock Granularity 
• Fine granularity locking (e.g., tuples) 
– High concurrency 
– High overhead in managing locks 
• Coarse grain locking (e.g., tables) 
– Many false conflicts 
– Less overhead in managing locks 
• Alternative techniques 
– Hierarchical locking (and intentional locks) [commercial DBMSs] 
– Lock escalation 


The Tree Protocol 
Rules: 
• A lock on a node A may only be acquired if the transaction holds a 
lock on its parent B 
• Nodes can be unlocked in any order (no 2PL necessary) 
• Cannot relock a node for which already released a lock 
• “Crabbing” 
– First lock parent then lock child 
– Keep parent locked only if may need to update it 
– Release lock on parent if child is not full
• The tree protocol is NOT 2PL, yet ensures conflict-serializability !


Optimistic Concurrency Control 
Validation-based technique 
• Phase 1: Read
– Transaction reads from database and writes to a private workspace 
• Phase 2: Validate 
– At commit time, system performs validation 
– Validation checks if transaction could have conflicted with others 
• Each transaction gets a timestamp 
• Check if timestamp order is equivalent to a serial order 
– If there is a potential conflict: abort 
• Phase 3: Write 
– If no conflict, transaction changes are copied into database


Optimistic Concurrency Control 
Timestamp-based technique 
• Each object, O, has read and write timestamps: RTS(O) and WTS(O) 
• Each transaction, T, has a timestamp TS(T) 
• Transaction wants to read object O
– If TS(T) < WTS(O) abort 
– Else read and update RTS(O) to larger of TS(T) or RTS(O) 
• Transaction wants to write object O
– If TS(T) < RTS(O) abort 
– If TS(T) < WTS(O) ignore my write and continue (Thomas Write Rule) 
– Otherwise, write O and update WTS(O) to TS(T) 


Multiversion-based technique 
• Object timestamps: RTS(O) & WTS(O); transaction timestamps TS(T) 
• Transaction can read most recent version that precedes TS(T) 
– When reading object, update RTS(O) to larger of TS(T) or RTS(O) 
• Transaction wants to write object O
– If TS(T) < RTS(O) abort 
– Otherwise, create a new version of O with WTS(O) = TS(T) 
• Common variant (used in commercial systems) 
– To write object O only check for conflicting writes not reads 
– Use locks for writes to avoid aborting in case conflicting transaction aborts


Buffer Manager Policies 
• STEAL or NO-STEAL
– Can an update made by an uncommitted transaction overwrite the most 
recent committed value of a data item on disk? 
• FORCE or NO-FORCE
– Should all updates of a transaction be forced to disk before the 
transaction commits? 
• Easiest for recovery: NO-STEAL/FORCE 
• Highest performance: STEAL/NO-FORCE 


Solution: Use a Log 
• Log: append-only file containing log records
• Enables the use of STEAL and NO-FORCE 
• For every update, commit, or abort operation 
– Write physical, logical, or physiological log record 
– Note: multiple transactions run concurrently, log records are 
interleaved 
• After a system crash, use log to: 
– Redo some transaction that did commit 
– Undo other transactions that didn’t commit 


Write-Ahead Log 
• All log records pertaining to a page are written to disk 
before the page is overwritten on disk 
• All log records for transaction are written to disk before
the transaction is considered committed 
– Why is this faster than FORCE policy? 
• Committed transaction: transactions whose commit log 
record has been written to disk


ARIES Method 
• Write-Ahead Log 
• Three pass algorithm 
– Analysis pass 
• Figure out what was going on at time of crash 
• List of dirty pages and active transactions 
– Redo pass (repeating history principle)
• Redo all operations, even for transactions that will not commit 
• Get back to state at the moment of the crash 
– Undo pass
• Remove effects of all uncommitted transactions 
• Log changes during undo in case of another crash during undo


log sequence number (LSN)

performance metrics for parallel dbms
• Speedup 
– More processors  higher speed
• Scalup
– More processors  can process more data 
– Transaction scaleup vs batch scaleup
• Challenges to speedup and scalup
– Startup cost: cost of starting an operation on many processors 
– Interference: contention for resources between processors 
– Skew: slowest step becomes the bottleneck


Architectures for Parallel Databases 
• Shared memory 
• Shared disk 
• Shared nothing 


Taxonomy for 
Parallel Query Evaluation 
• Inter-query parallelism 
– Each query runs on one processor 
• Inter-operator parallelism 
– A query runs on multiple processors 
– An operator runs on one processor 
• Intra-operator parallelism 
– An operator runs on multiple processors 
13 
We study only intra-operator parallelism: most scalable


Properties of a consensus protocol
• A consensus protocol provides safety if...
- Agreement – All outputs produced have the same value, and
- Validity – The output value equals one of the agents’ inputs
• A consensus protocol provides liveness if...
- Termination – Eventually non-failed agents output a value
• A consensus protocol provides fault tolerance if...
- It can survive the failure of an agent at any point
- Fail-stop protocols handle agent crashes
- Byzantine-fault-tolerant protocols handle arbitrary agent behavior
Theorem (FLP impossibility result)
No deterministic consensus protocol provides all three of safety,
liveness, and fault tolerance in an asynchronous system.



Hashing-Sorting solves “all” known data scale problems :=)


================================================================


# mysql
mysql queris and optimization
99+1 Tips to MySQL Tuning and Optimization
 

 

MySQL is a powerful open-source database.  With more and more database driven applications, people have been pushing MySQL to its limits.  Here are 101 tips for tuning and optimizing your MySQL install.  Some tips are specific to the environment they are installed on, but the concepts are universal.   I have divided them up into several categories to help you with getting the most out of MySQL:
 

transaction monitoring

MySQL Server Hardware and OS Tuning:
1. Have enough physical memory to load your entire InnoDB file into memory – InnoDB is much faster when the file can be accessed in memory rather than from disk.
2. Avoid Swap at all costs – swapping is reading from disk, its slow.
3. Use Battery-Backed RAM.
4. Use an advanced RAID – preferably RAID10 or higher.
5. Avoid RAID5 – the checksum needed to ensure integrity is costly.
6. Separate your OS and data partitions, not just logically, but physically – costly OS writes and reads will impact your database performance.
7. Put your mysql temp space and replication logs on a separate partition than your data – background writes will impact your database when it goes to write/read from disk.
8. More disks equals more speed.
9. Faster disks are better.
10. Use SAS over SATA.
11. Smaller disks are faster than larger disks, especially in RAID configs.
12. Use Battery-Backed Cache RAID controllers.
13. Avoid software raids.
14. Consider using Solid State IO Cards (not disk drives) for your data partition – these cards can sustain over 2GB/s writes for almost any amount of data.
15. On Linux set your swappiness value to 0 – no reason to cache files on a database server, this is more of a web server or desktop advantage.
16. Mount filesystem with noatime and nodirtime if available – no reason to update database file modification times for access.
17. Use XFS filesystem – a faster, smaller filesystem than ext3 and has more options for journaling, also ext3 has been shown to have double buffering issues with MySQL.
18. Tune your XFS filesystem log and buffer variables – for maximum performance benchmark.
19. On Linux systems, use NOOP or DEADLINE IO scheduler – the CFQ and ANTICIPATORY scheduler have been shown to be slow vs NOOP and DEADLINE scheduler.
20. Use a 64-bit OS – more memory addressable and usable to MySQL.
21. Remove unused packages and daemons from servers – less resource stealing.
22. Put your host that use MySQL and your MySQL host in a hosts file – no dns lookups.
23. Never force kill a MySQL process – you will corrupt your database and be running for the backups.
24. Dedicate your server to MySQL – background processes and other services can steal from the db cpu time.

MySQL Configuration:
25. Use innodb_flush_method=O_DIRECT to avoid a double buffer when writing.

26. Avoid O_DIRECT and EXT3 filesystem – you will serialize all your writes.

27. Allocate enough innodb_buffer_pool_size to load your entire InnoDB file into memory – less reads from disk.

28. Do not make innodb_log_file_size too big, with faster and more disks – flushing more often is good and lowers the recovery time during crashes.

29. Do not mix innodb_thread_concurrency and thread_concurrency variables – these two values are not compatible.

30. Allocate a minimal amount for max_connections – too many connections can use up your RAM and lock up your MySQL server.

31. Keep thread_cache at a relatively high number, about 16 – to prevent slowness when opening connections.

32. Use  skip-name-resolve – to remove dns lookups.

33. Use query cache if your queries are repetitive and your data does not change often – however using query cache on data that changes often will give you a performance hit.

34. Increase temp_table_size – to prevent disk writes.

35. Increase max_heap_table_size – to prevent disk writes.

36. Do not set your sort_buffer_size too high – this is per connection and can use up memory fast.

37. Monitor key_read_requests and key_reads to determine your key_buffer size – the key read requests should be higher than your key_reads, otherwise you are not efficiently using your key_buffer.

38. Set innodb_flush_log_at_trx_commit = 0 will improve performance, but leaving it to default (1), you will ensure data integrity, you will also ensure replication is not lagging

39. Have a test environment where you can test your configs and restart often, without affecting production.

MySQL Schema Optimization:

40. Keep your database trim.

41. Archive old data – to remove excessive row returns or searches on queries.

42. Put indexes on your data.

43. Do not overuse indexes, compare with your queries.

44. Compress text and blob data types – to save space and reduce number of disk reads.

45. UTF 8 and UTF16 is slower than latin1.


46. Use Triggers sparingly.

47. Keep redundant data to a minimum – do not duplicate data unnecessarily.

48. Use linking tables rather than extending rows.

49. Pay attention to your data types, use the smallest one possible for your real 
data.

50. Separate blob/text data from other data if other data is often used for queries when blob/text are not.

51. Check and optimize tables often.

52. Rewrite InnoDB tables often to optimize.

53. Sometimes, it is faster to drop indexes when adding columns and then add indexes back.

54. Use different storage engines for different needs.

55. Use ARCHIVE storage engine for Logging tables or Auditing tables – this is much more efficient for writes.

56. Store session data in memcache rather than MySQL – memcache allows for auto-expiring values and prevents you from having to create costly reads and writes to MySQL for temporal data.

57. Use VARCHAR instead CHAR when storing variable length strings – to save space since CHAR is fixed length and VARCHAR is not (utf8 is not affected by this).

58. Make schema changes incrementally – a small change can have drastic effects.

59. Test all schema changes in a development environment that mirrors production.

60. Do NOT arbitrarily change values in your config file, it can have disastrous affects.

61. Sometimes less is more in MySQL configs.

62. When in doubt use a generic MySQL config file.
MySQL metrics widget
Query Optimization:

63. Use the slow query log to find slow queries.

64. Use EXPLAIN to determine queries are functioning appropriately.

65. Test your queries often to see if they are performing optimally – performance 
will change over time.

66. Avoid count(*) on entire tables, it can lock the entire table.

67. Make queries uniform so subsequent similar queries will use query cache.

68. Use GROUP BY instead of DISTINCT when appropriate.

69. Use indexed columns in WHERE, GROUP BY, and ORDER BY clauses.

70. Keep indexes simple, do not reuse a column in multiple indexes.

71. Sometimes MySQL chooses the wrong index, use USE INDEX for this case

72. Check for issues using SQL_MODE=STRICT.

73. Use a LIMIT on UNION instead of OR for less than 5 indexed fields.

74. Use INSERT ON DUPLICATE KEY or INSERT IGNORE instead of UPDATE to avoid the 
SELECT prior to update.

75. Use a indexed field and ORDER BY instead of MAX.

76. Avoid using ORDER BY RAND().

77. LIMIT M,N can actually slow down queries in certain circumstances, use sparingly.

78. Use UNION instead of sub-queries in WHERE clauses.

79. For UPDATES, use SHARE MODE to prevent exclusive locks.

80. On restarts of MySQL, remember to warm your database, to ensure that your data is 
in memory and queries are fast.

81. Use DROP TABLE then CREATE TABLE instead of DELETE FROM to remove all data from a table.

82. Minimize the data in your query to only the data you need, using * is overkill 
most of the time.

83. Consider persistent connections instead of multiple connections to reduce overhead.

84. Benchmark queries, including using load on the server, sometimes a simple query can have affects on other queries.

85. When load increases on your server, use SHOW PROCESSLIST to view slow/problematic queries.

86. Test all suspect queries in a development environment where you have mirrored production data.
MySQL Backup Procedures:

87. Backup from secondary replicated server.

88. Stop replication during backups to prevent inconsistencies on data dependencies and foreign constraints.

89. Stop MySQL altogether and take a backup of the database files.

90. Backup binary logs at same time as dumpfile if MySQL dump used – to make sure replication does not break.

91. Do not trust an LVM snapshot for backups – this could create data inconsistencies that will give you issues in the future.

92. Make dumps per table for easier single table recovery – if data is isolated from other tables.

93. Use –opt when using mysqldump.

94. Check and Optimize tables before a backup.

95. When importing temporarily disable foreign constraints for a faster import.

96. When importing temporarily disable unique checks for a faster import.

97. Calculate size of database/tables data and indexes after each backup to monitor growth.

98. Monitor slave replication for errors and delay with a cron script.

99. Perform Backups regularly.

100. Test your backups regularly.



References:

Cross join:
SELECT *
FROM EMPLOYEE, COMPENSATION ;
Or
SELECT *
FROM EMPLOYEE CROSS JOIN COMPENSATION ;

STRAIGHT_JOIN:
SELECT table112.id,table112.bval1,table112.bval2,
table111.id,table111.aval1
FROM table112
STRAIGHT_JOIN table111;
 Or
It behaves like a inner join if given any condition.

Natural Join:
SELECT E.*, C.Salary, C.Bonus
 FROM EMPLOYEE E NATURAL JOIN COMPENSATION C ;

Or
SELECT E.*, C.Salary, C.Bonus
 FROM EMPLOYEE E, COMPENSATION C
 WHERE E.EmpID = C.EmpID ;


Condition Join:
Example: using ON aggretated function
SELECT *
 FROM NATIONAL JOIN AMERICAN
 ON NATIONAL.CompleteGames = AMERICAN.CompleteGames ;

SQL Predection:

ALL
BETWEEN
DISTINCT
EXISTS
IN
LIKE
MATCH
NOT IN
NOT LIKE
NULL
OVERLAPS
SOME, ANY
UNIQUE



Normalities:
First Normal Form (1NF):
Table must be two-dimensional, with rows and columns.
Each row contains data that pertains to one thing or one portion of a thing.
Each column contains data for a single attribute of the thing being described.
Each cell (intersection of row and column) of the table must be single-valued.
All entries in a column must be of the same kind.
Each column must have a unique name.
No two rows may be identical.
The order of the columns and of the rows does not matter.
Second Normal Form (2NF):
Table must be in first normal form (1NF).
All non-key attributes (columns) must be dependent on the entire key.
Third Normal Form (3NF):
Table must be in second normal form (2NF).
Table has no transitive dependencies.
Domain-Key Normal Form (DK/NF):
Every constraint on the table is a logical consequence of the definition of keys and domains.

SET FUNCTIONS & AGGREGATE FUNCTIONS:

COUNT
Returns the number of rows in the specified table
MAX
Returns the maximum value that occurs in the specified able
MIN
Returns the minimum value that occurs in the specified table
SUM
Adds up the values in a specified column
AVG
Returns the average of all the values in the specified column


TRIGONOMETRIC AND LOGARITHMIC FUNCTIONS

sin, cos, tan, asin, acos, atan, sinh, cosh, tanh, log(<base>, <value>), log10(<value>). ln( <value>)

JSON CONSTRUCTOR FUNCTIONS
JSON_OBJECT
JSON_ARRAY
JSON_OBJECTAGG
JSON_ARRAYAGG
JSON QUERY FUNCTIONS
JSON_EXISTS
JSON_VALUE
JSON_QUERY
JSON_TABLE
DATA TYPES:
Here’s a list of all the formal data types that ISO/IEC standard SQL recognizes. In addition to these, you may define additional data types that are derived from these.
Exact Numerics:
INTEGER
SMALLINT
BIGINT
NUMERIC
DECIMAL
Approximate Numerics:
REAL
DOUBLE PRECISION
FLOAT
DECFLOAT
Binary Strings:
BINARY
BINARY VARYING
BINARY LARGE OBJECT
Boolean:
BOOLEAN
Character Strings:
CHARACTER
CHARACTER VARYING (VARCHAR)
CHARACTER LARGE OBJECT
NATIONAL CHARACTER
NATIONAL CHARACTER VARYING
NATIONAL CHARACTER LARGE OBJECT
Datetimes:
DATE
TIME WITHOUT TIMEZONE
TIMESTAMP WITHOUT TIMEZONE
TIME WITH TIMEZONE
TIMESTAMP WITH TIMEZONE
Intervals:
INTERVAL DAY
INTERVAL YEAR
Collection Types:
ARRAY
MULTISET
Other Types:
ROW
XML


DBMS RAMKRISHNA:
Levels of Abstraction in a DBMS:
Conceptual
Physical
External
data definition language (DDL) is used to define the external and conceptual schemas
Physical schema specifies additional storage details
External schemas, which usually are also in terms of the data model of the DBMS, allow data access to be customized (and authorized) at the level of individual users or groups of users
DATABASE DESIGN AND ER DIAGRAMS:
 Requirements Analysis
Conceptual Database Design
Logical Database Design
Schema Refinement
Physical Database Design | performance
Application and Security Design

ENTITIES, ATTRIBUTES, AND ENTITY SETS:
RELTIONSHIPS AND RELATIONSHIP SETS:
Key Constraints
Key Constraints for Ternary Relationships
Weak Entities
Class Hierarchies
Aggregation
Entity versus Attribute
THE RELATIONAL MODEL:
CASCADE:
The CASCADE option ensures that information about an employee's policy and dependents is deleted if the corresponding Employees tuple is deleted. The Relational 1"1,,1oriel 3.5.6 T
FOREIGN KEY (ssn) REFERENCES Employees ON DELETE CASCADE )
INTRODUCTION TO VIEWS:
A view is a table whose rows are not explicitly stored in the database but are computed as needed from a view definition
Views, Data Independence, Security
RELATIONAL ALGEBRA AND CALCULUS:
SET OPERATIONS:
Union
Intersection
Difference
Cross product
Division (oppo of Joins)
Distinct
RELATIONAL CALCULUS:
The variant of the calculus we present in detail is called the tuple relational calculus (TRC)
Select * from table where(# relational calculus)

SQL: QUERIES, CONSTRAINTS, TRIGGERS:
 Triggers and Advanced Integrity Constraints:
Correlated Nested Queries:
Pind the names of sailors who have reserved boat nv,mber 103. 
SELECT FROM WHERE S.sname Sailors S EXISTS ( SELECT * FROM Reserves R WHERE R.bid = 103 AND R.sid = S.sid )
Set-Comparison Operators:
Find sailors whose rating is better than some sailor called Horatio. 
SELECT S.sid FROM Sailors S WHERE S.rating > ANY ( SELECT FROM WHERE S2.rating Sailors S2 S2.sname = 'Horatio' )
The GROUP BY and HAVING Clauses
STORAGE AND INDEXING:
INDEX DATA STRUCTURES:
One way to organize data entries is to hash data entries on the sea.rch key
Tree-Based Indexing:
 An alternative to hash-based indexing is to organize records using a treelike data structure. The data entries are arranged in sorted order by search key value, and a hierarchical search data structure is maintained that directs searches to the correct page of data entries.
COMPARISON OF FILE ORGANIZATIONS:


Exporting Data with the SELECT ... INTO OUTFILE Statement

SELECT * FROM passwd INTO OUTFILE '/tmp/tutorials.txt'
   -> FIELDS TERMINATED BY ',' ENCLOSED BY '"'
   -> LINES TERMINATED BY '\r\n';

mysqldump -u root -p database_name table_name > dump.txt
password *****
 IMPORT:
LOAD DATA LOCAL INFILE 'dump.txt' INTO TABLE mytbl;
mysqlimport -u root -p --local --fields-terminated-by = ":" \
   --lines-terminated-by = "\r\n"  database_name dump.txt
password *****

The SQL IN Operator
The IN operator allows you to specify multiple values in a WHERE clause.
The IN operator is a shorthand for multiple OR conditions.

Case:
SELECT OrderID, Quantity,
CASE
    WHEN Quantity > 30 THEN "The quantity is greater than 30"
    WHEN Quantity = 30 THEN "The quantity is 30"
    ELSE "The quantity is under 30"
END
FROM OrderDetails;

The MySQL IFNULL() function lets you return an alternative value if an expression is NULL:
SELECT ProductName, UnitPrice * (UnitsInStock + IFNULL(UnitsOnOrder, 0))
FROM Products

Stored Procedure Syntax
CREATE PROCEDURE procedure_name
AS
sql_statement
GO;

Execute a Stored Procedure
EXEC procedure_name;
With param:
CREATE PROCEDURE SelectAllCustomers @City nvarchar(30)
AS
SELECT * FROM Customers WHERE City = @City
GO;
EXEC SelectAllCustomers City = "London";

Introduction to MySQL ENUM data type
In MySQL, an ENUM is a string object whose value is chosen from a list of permitted values defined at the time of column creation.

CREATE TABLE table_name (
    ...
    col ENUM ('value1','value2','value3'),
    ...
);

CREATE TABLE tickets (
    id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(255) NOT NULL,
    priority ENUM('Low', 'Medium', 'High') NOT NULL
);
INSERT INTO tickets(title, priority)
VALUES('Scan virus for computer A', 'High');

Cascade 2:

CREATE TABLE rooms (
    room_no INT PRIMARY KEY AUTO_INCREMENT,
    room_name VARCHAR(255) NOT NULL,
    building_no INT NOT NULL,
    FOREIGN KEY (building_no)
        REFERENCES buildings (building_no)
        ON DELETE CASCADE
);


Mysql json:
CREATE TABLE table_name (
    ...
    json_column_name JSON,  
    ...
);

Triggers:


2
3
4
5
6
7
8
DELIMITER $$
CREATE TRIGGER  trigger_name
[BEFORE|AFTER] [INSERT|UPDATE|DELETE] ON table_name
FOR EACH ROW [FOLLOWS|PRECEDES] existing_trigger_name
BEGIN
…
END$$
DELIMITER ;


DELIMITER $$
 
CREATE TRIGGER before_products_update
   BEFORE UPDATE ON products
   FOR EACH ROW
BEGIN
    INSERT INTO price_logs(product_code,price)
    VALUES(old.productCode,old.msrp);
END$$
 
DELIMITER ;


Node.js         49.6%
Angular         36.9%
React               27.8%
Spring          17.6%
Django          13.0%

Cordova         8.5%            Spark               4.8%
Hadoop          4.7%            TensorFlow          7.8%
Xamarin         7.4%            Torch/PyTorch       1.7%

MySQL
58.6%
SQL Server
41.6%
PostgreSQL
33.3%
MongoDB
26.4%
SQLite
19.7%
Redis
18.5%
Elasticsearch
14.4%
MariaDB
13.5%
Oracle
11.1%
Microsoft Azure (Tables, CosmosDB, SQL, etc)
8.0%
Memcached
5.6%
Google Cloud Storage
5.5%
Amazon DynamoDB
5.3%
Amazon RDS/Aurora
5.2%
Cassandra
3.7%
IBM Db2
2.5%
Neo4j
2.4%
Amazon Redshift
2.2%
Apache Hive
2.2%
Google BigQuery
2.1%
Apache HBase
1.7%

=============================================



Covering Indexes


Slow Query Basics: Optimize Data Access
The most basic reason a query doesn’t perform well is because it’s working with too
much data. Some queries just have to sift through a lot of data, which can’t be helped.
That’s unusual, though; most bad queries can be changed to access less data. We’ve
found it useful to analyze a poorly performing query in two steps:
1. Find out whether your application is retrieving more data than you need. That
usually means it’s accessing too many rows, but it might also be accessing too
many columns.
2. Find out whether the MySQL server is analyzing more rows than it needs.



Here are a few typical mistakes:

Fetching more rows than needed
Fetching all columns from a multitable join
Fetching all columns
Fetching the same data repeatedly


 In MySQL, the simplest
query cost metrics are:
• Response time
• Number of rows examined
• Number of rows returned



If you find that a huge number of rows were examined to produce relatively few rows
in the result, you can try some more sophisticated fixes:
• Use covering indexes, which store data so that the storage engine doesn’t have to
retrieve the complete rows. (We discussed these in Chapter 7.)
• Change the schema. An example is using summary tables (discussed in
Chapter 6).
• Rewrite a complicated query so the MySQL optimizer is able to execute it opti‐
mally. (We discuss this later in this chapter.)



At a high level, replication is a simple three-part process:
1. The source records changes to its data in its binary log as “binary log events.”
2. The replica copies the source’s binary log events to its own local relay log.
3. The replica replays the events in the relay log, applying the changes to its own
data.




Fundamental operations to retrieve
and manipulate tuples in a relation.
→ Based on set algebra.
Each operator takes one or more
relations as its inputs and outputs a
new relation.
→ We can "chain" operators together to create
more complex operations.


Data Manipulation Language (DML)
Data Definition Language (DDL)
Data Control Language (DCL)



group by 
Project tuples into subsets and 
calculate aggregates against
each subset.


Non-aggregated values in SELECT output clause 
must appear in GROUP BY clause.

having 对 group by 进行过滤
Filters results based on aggregation computation.
Like a WHERE clause for a GROUP BY


NESTED QUERIES
Queries containing other queries.
They are often difficult to optimize. 
Inner queries can appear (almost) anywhere in 
query.



ALL→ Must satisfy expression for all rows in the 
sub-query.
ANY→ Must satisfy expression for at least one row 
in the sub-query.
IN→ Equivalent to '=ANY()' .
EXISTS→ At least one row is returned.


SEQUENTIAL VS. RANDOM ACCESS


SYSTEM DESIGN GOALS
Allow the DBMS to manage databases that exceed 
the amount of memory available.
Reading/writing to disk is expensive, so it must be 
managed carefully to avoid large stalls and 
performance degradation.
Random access on disk is usually much slower 
than sequential access, so the DBMS will want to 
maximize sequential access.


WHY NOT USE THE OS?
There are some solutions to this 
problem:
→ madvise: Tell the OS how you expect to 
read certain pages.
→ mlock: Tell the OS that memory ranges 
cannot be paged out.
→ msync: Tell the OS to flush memory 
ranges out to disk.



DBMS (almost) always wants to control things 
itself and can do a better job at it.
→ Flushing dirty pages to disk in the correct order.
→ Specialized prefetching.
→ Buffer replacement policy.
→ Thread/process scheduling.
The OS is not your friend



重点2 database storage
Problem #1: How the DBMS represents the 
database in files on disk.
Problem #2: How the DBMS manages its memory 
and move data back-and-forth from disk.



The DBMS stores a database as one or more files 
on disk typically in a proprietary format.
→ The OS doesn't know anything about the contents of 
these files.
Early systems in the 1980s used custom filesystems 
on raw storage.
→ Some "enterprise" DBMSs still support this.
→ Most newer DBMSs do not do this.
数据库有用os文件系统的， 有自制文件系统的


The storage manager is responsible for 
maintaining a database's files.
→ Some do their own scheduling for reads and writes to 
improve spatial and temporal locality of pages.
It organizes the files as a collection of pages.
→ Tracks data read/written to pages.
→ Tracks the available space.


A page is a fixed-size block of data.
→ It can contain tuples, meta-data, indexes, log records…
→ Most systems do not mix page types.
→ Some systems require a page to be self-contained.
Each page is given a unique identifier.
→ The DBMS uses an indirection layer to map page ids to 
physical locations.


HEAP FILE: LINKED LIST
Maintain a header page at the 
beginning of the file that stores two 
pointers:
→ HEAD of the free page list.
→ HEAD of the data page list.
Each page keeps track of how many 
free slots they currently have.

数据库heap文件页的第一页为页统计状态page文件
HEAP FILE: PAGE DIRECTORY
The DBMS maintains special pages 
that tracks the location of data pages 
in the database files.
The directory also records the number 
of free slots per page.
The DBMS must make sure that the 
directory pages are in sync with the 
data pages.



PAGE LAYOUT
For any page storage architecture, we now need to 
decide how to organize the data inside of the page.
→ We are still assuming that we are only storing tuples.
How to store tuples in a page?
Strawman Idea: Keep track of the 
number of tuples in a page and then 
just append a new tuple to the end.
→ What happens if we delete a tuple?
→ What happens if we have a variablelength attribute?



把 tuple加到后边， 
删除可能涉及顺序 重整 
可变长page前是索引 page后边是tuple, tuple从后往前， 索引从前往后。 


The DBMS needs a way to keep track 
of individual tuples.
Each tuple is assigned a unique record 
identifier.
→ Most common: page_id + offset/slot
→ Can also contain file location info. 
An application cannot rely on these 
ids to mean anything.

反正规化， 把相干的关系放在一起
Can physically denormalize (e.g., "pre 
join") related tuples and store them 
together in the same page.
→ Potentially reduces the amount of I/O for 
common workload patterns.
→ Can make updates more expensive.



Not a new idea.
→ IBM System R did this in the 1970s.
→ Several NoSQL DBMSs do this without 
calling it physical denormalization.


日志型文件组织形式
LOG-STRUCTURED FILE ORGANIZATION
Instead of storing tuples in pages, the 
DBMS only stores log records.
The system appends log records to the 
file of how the database was modified:
→ Inserts store the entire tuple.
→ Deletes mark the tuple as deleted.
→ Updates contain the delta of just the 
attributes that were modified.


To read a record, the DBMS scans the 
log backwards and "recreates" the 
tuple to find what it needs.
Build indexes to allow it to jump to 
locations in the log.
Periodically compact the log.

周期性压缩日志

hbase cassandra, leveldb, rocksdb

大类型没保护
EXTERNAL VALUE STORAGE
Some systems allow you to store a 
really large value in an external file.
Treated as a BLOB type.
→ Oracle: BFILE data type
→ Microsoft: FILESTREAM data type
The DBMS cannot manipulate the 
contents of an external file.
→ No durability protections.
→ No transaction protections.



A DBMS stores meta-data about databases in its 
internal catalogs.
→ Tables, columns, indexes, views
→ Users, permissions
→ Internal statistics
Almost every DBMS stores databases' catalogs in 
another database.
→ Wrap object abstraction around tuples.
→ Specialized code for "bootstrapping" catalog tables.


On-Line Transaction Processing (OLTP)
→ Fast operations that only read/update a small amount of 
data each time. 
On-Line Analytical Processing (OLAP)
→ Complex queries that read a lot of data to compute 
aggregates.
Hybrid Transaction + Analytical Processing
→ OLTP + OLAP together on the same database instance


Open-source
Apache Pinot is used at LinkedIn, Cisco, Uber, Slack, Stripe, DoorDash, Target, Walmart, Amazon, and Microsoft to deliver scalable real time analytics with low latency.[30] It can ingest data from offline data sources (such as Hadoop and flat files) as well as online sources (such as Kafka). Pinot is designed to scale horizontally.
Mondrian OLAP server is an open-source OLAP server written in Java. It supports the MDX query language, the XML for Analysis and the olap4j interface specifications.
Apache Druid is a popular open-source distributed data store for OLAP queries that is used at scale in production by various organizations.
Apache Kylin is a distributed data store for OLAP queries originally developed by eBay.
Cubes (OLAP server) is another light-weight open-source toolkit implementation of OLAP functionality in the Python programming language with built-in ROLAP.
ClickHouse is a fairly new column orientated DBMS focusing on fast processing and response times.

N-ARY storage model：
The DBMS stores all attributes for a single tuple 
contiguously in a page.

Advantages
→ Fast inserts, updates, and deletes.
→ Good for queries that need the entire tuple.
Disadvantages
→ Not good for scanning large portions of the table and/or 
a subset of the attributes.


Advantages
→ Reduces the amount wasted I/O because the DBMS only 
reads the data that it needs.
→ Better query processing and data compression (more on 
this later).
Disadvantages
→ Slow for point queries, inserts, updates, and deletes 
because of tuple splitting/stitching.


It is important to choose the right storage model 
for the target workload:
→ OLTP = Row Store
→ OLAP = Column Store


文件如何存放，文件顺序处理， 内存数据如何同步
DATABASE STORAGE
Spatial Control:
→ Where to write pages on disk.
→ The goal is to keep pages that are used together often as 
physically close together as possible on disk.
Temporal Control:
→ When to read pages into memory, and when to write 
them to disk.
→ The goal is minimize the number of stalls from having to 
read data from disk.




内存管理 缓存管理
Buffer Pool Manager
Replacement Policies
Other Memory Pools

page加载状态表
The page table keeps track of pages 
that are currently in memory.
Also maintains additional meta-data 
per page:
→ Dirty Flag
→ Pin/Reference Counter



LOCKS VS. L ATCHES


PAGE TABLE VS. PAGE DIRECTORY
The page directory is the mapping from page ids 
to page locations in the database files.
→ All changes must be recorded on disk to allow the DBMS 
to find on restart.
The page table is the mapping from page ids to a 
copy of the page in buffer pool frames.
→ This is an in-memory data structure that does not need to 
be stored on disk.


BUFFER POOL OPTIMIZATIONS
Multiple Buffer Pools
Pre-Fetching
Scan Sharing
Buffer Pool Bypass


Approach #1: Object Id
→ Embed an object identifier in record ids 
and then maintain a mapping from objects 
to specific buffer pools.
Approach #2: Hashing
→ Hash the page id to select which
buffer pool to access.

预加载， 把一个计划的相关页预加载， 预加载索引页
PRE-FETCHING
The DBMS can also prefetch pages 
based on a query plan.
→ Sequential Scans
→ Index Scans


相关的query分享page.
Queries can reuse data retrieved from storage or 
operator computations.
→ Also called synchronized scans.
→ This is different from result caching.
Allow multiple queries to attach to a single cursor 
that scans a table.
→ Queries do not have to be the same.
→ Can also share intermediate results.


SCAN SHARING
If a query wants to scan a table and another query 
is already doing this, then the DBMS will attach 
the second query's cursor to the existing cursor.
Examples:
→ Fully supported in IBM DB2, MSSQL, and Postgres.
→ Oracle only supports cursor sharing for identical queries.


跳过  os page cache. 
Most DBMSs use direct I/O (O_DIRECT)to bypass 
the OS's cache.
→ Redundant copies of pages.
→ Different eviction policies.

page缓存策略
LEAST-RECENTLY USED
Maintain a single timestamp of when each page 
was last accessed.
When the DBMS needs to evict a page, select the 
one with the oldest timestamp.
→ Keep the pages in sorted order to reduce the search time 
on eviction.


缓存暗示
BETTER POLICIES: PRIORITY HINTS
The DBMS knows what the context of each page 
during query execution.
It can provide hints to the buffer pool on whether 
a page is important or not.


BACKGROUND WRITING
The DBMS can periodically walk through the page 
table and write dirty pages to disk.
When a dirty page is safely written, the DBMS can 
either evict the page or just unset the dirty flag.
Need to be careful that we don’t write dirty pages 
before their log records have been written…


OTHER MEMORY POOLS
The DBMS needs memory for things other than 
just tuples and indexes.
These other memory pools may not always backed
by disk. Depends on implementation.
→ Sorting + Join Buffers
→ Query Caches
→ Maintenance Buffers
→ Log Buffers
→ Dictionary Caches


代码质量
CODE QUALITY
We will automatically check whether you are 
writing good code.
→ Google C++ Style Guide
→ Doxygen Javadoc Style
You need to run these targets before you submit 
your implementation to Gradescope.



数据库五大部分从底到顶
disk manager
buffer pool manager 
access methods
operator execution
query planning



HASH TABLE
Design Decision #1: Hash Function
→ How to map a large key space into a smaller domain.
→ Trade-off between being fast vs. collision rate.
Design Decision #2: Hashing Scheme
→ How to handle key collisions after hashing.
→ Trade-off between allocating a large hash table vs. 
additional instructions to find/insert keys.



For any input key, return an integer 
representation of that key.
We do not want to use a cryptographic hash 
function for DBMS hash tables.
We want something that is fast and has a low 
collision rate.

hash 算法进化
HASH FUNCTIONS
CRC-64 (1975)
→ Used in networking for error detection.
MurmurHash (2008)
→ Designed to a fast, general purpose hash function.
Google CityHash (2011)
→ Designed to be faster for short keys (<64 bytes).
Facebook XXHash (2012)
→ From the creator of zstd compression.
Google FarmHash (2014)
→ Newer version of CityHash with better collision rates


STATIC HASHING SCHEMES
Approach #1: Linear Probe Hashing  向下找， 一直向下
Approach #2: Robin Hood Hashing   
罗宾汉哈希法（robin hood hashing）是线性探查法的变种，可以避免线性探查法中可能出现的在连续区域里频繁后移的操作。当我们进行插入时，如果发现单元格被其他键值对占用，那么就需要比较这俩个键距离其原本位置的距离。距离较远的键值对留下，距离较近的被迫后移。
Approach #3: Cuckoo Hashing
多hashing 方法

动态hashing
The previous hash tables require the DBMS to 
know the number of elements it wants to store.
→ Otherwise it must rebuild the table if it needs to 
grow/shrink in size.
Dynamic hash tables resize themselves on demand.
→ Chained Hashing
→ Extendible Hashing
动态扩展
→ Linear Hashing


EXTENDIBLE HASHING


Fast data structures that support O(1) look-ups that 
are used all throughout the DBMS internals.
→ Trade-off between speed and flexibility.
Hash tables are usually not what you want to use 
for a table index…


CockroachDB Query Optimizer
→ Monday Sept 28th @ 5pm ET
Apache Arrow
→ Monday Oct 5th @ 5pm ET
DataBricks Query Optimizer
→ Monday Oct 12th @ 5pm ET

索引 
A table index is a replica of a subset of a table's 
attributes that are organized and/or sorted for 
efficient access using a subset of those attributes.
The DBMS ensures that the contents of the table 
and the index are logically in sync.

索引注意事项
There is a trade-off on the number of indexes to 
create per database.
→ Storage Overhead
→ Maintenance Overhead、


页结点  

Approach #1: Record Ids
→ A pointer to the location of the tuple that 
the index entry corresponds to.
Approach #2: Tuple Data
→ The actual contents of the tuple is stored 
in the leaf node.
→ Secondary indexes must store the record 
id as their values.

B+树插入方法
Find correct leaf node L.
Put data entry into L in sorted order.
If L has enough space, done!
Otherwise, split L keys into L and a new node L2
→ Redistribute entries evenly, copy up middle key.
→ Insert index entry pointing to L2 into parent of L.
To split inner node, redistribute entries evenly, 
but push up middle key.


B+TREE DUPLICATE KEYS
Approach #1: Append Record Id
→ Add the tuple's unique record id as part of the key to 
ensure that all keys are unique.
→ The DBMS can still use partial keys to find tuples.
Approach #2: Overflow Leaf Nodes
→ Allow leaf nodes to spill into overflow nodes that contain 
the duplicate keys.
→ This is more complex to maintain and modif


数据库主键实现
CLUSTERED INDEXES
The table is stored in the sort order specified by 
the primary key.
→ Can be either heap- or index-organized storage.
Some DBMSs always use a clustered index.
→ If a table does not contain a primary key, the DBMS will 
automatically make a hidden row id primary key.
Other DBMSs cannot use them at all.



The venerable B+Tree is always a good choice for 
your DBMS.


MERGE THRESHOLD
Some DBMSs do not always merge nodes when it 
is half full.
Delaying a merge operation may reduce the 
amount of reorganization.
It may also be better to just let underflows to exist 
and then periodically rebuild entire tree.

优化
OPTIMIZATIONS
Prefix Compression
共同前缀分离出来
Deduplication
    DEDUPLICATION
    Non-unique indexes can end up 
    storing keys multiple copies of the 
    same key in leaf nodes.
    → This will make more sense when we talk
    about MVCC.
    The leaf node can store the key once 
    and then a maintain a list of record ids 
    with that key.
    重复key

Suffix Truncation
    SUFFIX TRUNCATION
The keys in the inner nodes are only 
used to "direct traffic".
→ We don't need the entire key.
Store a minimum prefix that is needed 
to correctly route probes into the 
index.


Bulk Insert
    最快建索引的方法， 先排好序， 直接建
    BULK INSERT
    The fastest way to build a new 
    B+Tree for an existing table is to first 
    sort the keys and then build the index 
    from the bottom up.
Pointer Swizzling
Nodes use page ids to reference other 
nodes in the index. The DBMS must 
get the memory location from the 
page table during traversal.
If a page is pinned in the buffer pool, 
then we can store raw pointers 
instead of page ids. This avoids 
address lookups from the page table.

分索引 只对子集数据进行索引 ， 减少索引压力
PARTIAL INDEXES
Create an index on a subset of the 
entire table. This potentially reduces 
its size and the amount of overhead
to maintain it.
One common use case is to partition 
indexes by date ranges.
→ Create a separate index per month, year.


覆盖索引
COVERING INDEXES
If all the fields needed to process the 
query are available in an index, then 
the DBMS does not need to retrieve 
the tuple.
This reduces contention on the 
DBMS's buffer pool resources.
Also called index-only scans.

索引追加列， 加快select
INDEX INCLUDE COLUMNS
Embed additional columns in indexes 
to support index-only queries.
These extra columns are only stored 
in the leaf nodes and are not part of 
the search key.


字段函数加索引 
FUNCTIONAL/EXPRESSION INDEXES
An index does not need to store keys 
in the same way that they appear in 
their base table.
You can use expressions when 
declaring an index.


The inner node keys in a B+Tree cannot tell you 
whether a key exists in the index. You must always 
traverse to the leaf node.
This means that you could have (at least) one 
buffer pool page miss per level in the tree just to 
find out a key does not exist.


字母树
TRIE INDEX
Use a digital representation of keys to 
examine prefixes one-by-one instead 
of comparing entire key.
→ Also known as Digital Search Tree, Prefix 
Tree.

字母树优点
TRIE INDEX PROPERTIES
Shape only depends on key space and lengths.
→ Does not depend on existing keys or insertion order.
→ Does not require rebalancing operations.
All operations have O(k) complexity where k is the 
length of the key.
→ The path to a leaf node represents the key of the leaf
→ Keys are stored implicitly and can be reconstructed from 
paths.




trie key span




反向索引 ， 用于文本搜索
INVERTED INDEX
An inverted index stores a mapping of words to 
records that contain those words in the target 
attribute.
→ Sometimes called a full-text search index.
→ Also called a concordance in old (like really old) times.
The major DBMSs support these natively.
There are also specialized DBMSs.

xapian elasticsearch solr sphinx  lucene


QUERY T YPES
Phrase Searches
→ Find records that contain a list of words in the given 
order.
Proximity Searches
→ Find records where two words occur within n words of 
each other.
Wildcard Searches
→ Find records that contain words that match some pattern 
(e.g., regular expression).

其它树
CONCLUSION
B+Trees are still the way to go for tree indexes.
Inverted indexes are covered in CMU 11-442.
We did not discuss geo-spatial tree indexes:
→ Examples: R-Tree, Quad-Tree, KD-Tree
→ This is covered in CMU 15-826.


Search Engines slides edu
全文检索书藉
Textbook: The textbook is Introduction to Information Retrieval, Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze, Cambridge University Press. 2008. You may use the printed copy or the online copy, but note that the reading instructions refer to the printed copy.

索引异步控制 
Index Concurrency Control


多用户访问 解决冲突
We assumed that all the data structures that we 
have discussed so far are single-threaded.
But we need to allow multiple threads to safely 
access our data structures to take advantage of 
additional CPU cores and hide disk I/O stalls


voltdb, redis, h-store不解决这些问题

Concurrency is the execution of the multiple instruction sequences at the same time. It happens in the operating system when there are several process threads running in parallel. The running process threads always communicate with each other through shared memory or message passing.

解决并发对共享数据操作失误的问题
A concurrency control protocol is the method 
that the DBMS uses to ensure "correct" results for 
concurrent operations on a shared object.
A protocol's correctness criteria can vary:
→ Logical Correctness: Can a thread see the data that it is 
supposed to see?
→ Physical Correctness: Is the internal representation of 
the object sound?

栓可能用系统信号解决
Latches Overview
Hash Table Latching
B+Tree Latching
Leaf Node Scans
Delayed Parent Updates

Latches are used to guarantee physical consistency of data, while locks are used to assure logical consistency of data.

Latches are simple, low-level system lock (serialization mechanisms) that coordinate multi-user access (concurrency) to shared data structures, objects, and files.

Latches protect shared memory resources from corruption when accessed by multiple processes. Specifically, latches protect data structures from the following situations:

Concurrent modification by multiple sessions
Being read by one session while being modified by another session
Deallocation (aging out) of memory while being accessed
Typically, a single latch protects multiple objects in the SGA.


The implementation of latches is operating system-dependent, especially in respect to whether and how long a process waits for a latch.

As an auxiliary to locks, lighter-weight latches are also provided for mutual exclusion. Latches are more akin to monitors or semaphores than locks; they are used to provide exclusive access to internal data structures.

As an example in a database, the buffer pool page table has a latch associated with each frame, to guarantee that only one DBMS thread is replacing a given frame at any time. Latches are used in the implementation of locks and to briefly stabilize internal data structures potentially being concurrently modified.


A lock allows only one thread to enter the part that's locked and the lock is not shared with any other processes.

A mutex is the same as a lock but it can be system wide (shared by multiple processes).

A semaphore does the same as a mutex but allows x number of threads to enter, this can be used for example to limit the number of cpu, io or ram intensive tasks running at the same time.

For a more detailed post about the differences between mutex and semaphore read here.

You also have read/write locks that allows either unlimited number of readers or 1 writer at any given time.


Locks
→ Protects the database's logical contents from other txns.
→ Held for txn duration.
→ Need to be able to rollback changes.
Latches
→ Protects the critical sections of the DBMS's internal data 
structure from other threads.
→ Held for operation duration.
→ Do not need to be able to rollback changes.


Hopefully we now have a clear understanding of the difference between the binary semaphore and the counting semaphore. Before moving onto the mutex we need to understand the inherent dangers associated with using the semaphore. These include:

信号常见问题
Accidental release
Recursive deadlock
Task-Death deadlock
Priority inversion
Semaphore as a signal
All these problems occur at run-time and can be very difficult to reproduce; making technical support very difficult.


latch implementation 栓实现
Blocking OS Mutex
Test-and-Set Spinlock
Reader-Writer Locks

搜索图书推荐
Course Texts
There is no required textbook for this course. Readings will be suggested in the syllabus from the following two books.
Search Engines: Information Retrieval in Practice. Croft, Metzler and Strohman (Addison-Wesley, 2008).
Introduction to Information Retrieval. Manning, Raghavan, and Schütze (Cambridge). Available online.

what's mutex
When I am having a big heated discussion at work, I use a rubber chicken which I keep in my desk for just such occasions. The person holding the chicken is the only person who is allowed to talk. If you don't hold the chicken you cannot speak. You can only indicate that you want the chicken and wait until you get it before you speak. Once you have finished speaking, you can hand the chicken back to the moderator who will hand it to the next person to speak. This ensures that people do not speak over each other, and also have their own space to talk.

Replace Chicken with Mutex and person with thread and you basically have the concept of a mutex.

Of course, there is no such thing as a rubber mutex. Only rubber chicken. My cats once had a rubber mouse, but they ate it.

Of course, before you use the rubber chicken, you need to ask yourself whether you actually need 5 people in one room and would it not just be easier with one person in the room on their own doing all the work. Actually, this is just extending the analogy, but you get the idea.



Approach #2: Test-and-Set Spin Latch (TAS)
→ Very efficient (single instruction to latch/unlatch)
→ Non-scalable, not cache friendly, not OS friendly.
→ Example: std::atomic<T>

并发出错控制 
B+TREE CONCURRENCY CONTROL
We want to allow multiple threads to read and 
update a B+Tree at the same time.
We need to protect from two types of problems:
→ Threads trying to modify the contents of a node at the 
same time.
→ One thread traversing the tree while another thread 
splits/merges nodes.


L ATCH CRABBING/COUPLING
Protocol to allow multiple threads to 
access/modify B+Tree at the same time.
Basic Idea:
→ Get latch for parent.
→ Get latch for child
→ Release latch for parent if “safe”.
A safe node is one that will not split or merge 
when updated.
→ Not full (on insertion)
→ More than half-full (on deletion)


Find: Start at root and go down; repeatedly,
→ Acquire R latch on child
→ Then unlatch parent
Insert/Delete: Start at root and go down, 
obtaining W latches as needed. Once child is 
latched, check if it is safe:
→ If child is safe, release all latches on ancestors.

锁机制， 先父锁， 再取子锁， 判断操作是否影响父锁， 不影响释放他， 继续向下。 

上边的方法有个问题是影响效率
Taking a write latch on the root every time 
becomes a bottleneck with higher concurrency


更新操作的时候， 先用读锁， 如果发现不行， 再用写锁。 
Most modifications to a B+Tree will 
not require a split or merge.
Instead of assuming that there will be 
a split/merge, optimistically traverse 
the tree using read latches.
If you guess wrong, repeat traversal 
with the pessimistic algorithm.


Search: Same as before.
Insert/Delete: 
→ Set latches as if for search, get to leaf, and set W latch on 
leaf.
→ If leaf is not safe, release all latches, and restart thread 
using previous insert/delete protocol with write latches.
This approach optimistically assumes that only leaf 
node will be modified; if not, R latches set on the 
first pass to leaf are wasteful.

为了效率  延迟结构平衡
DELAYED PARENT UPDATES
Every time a leaf node overflows, we must update 
at least three nodes.
→ The leaf node being split.
→ The new leaf node being created.
→ The parent node.
B
link-Tree Optimization: When a leaf node 
overflows, delay updating its parent node.

锁加版本
VERSIONED LATCH COUPLING
Optimistic crabbing scheme where writers are not 
blocked on readers.
Every node now has a version number (counter).
→ Writers increment counter when they acquire latch.
→ Readers proceed if a node’s latch is available but then do 
not acquire it.
→ It then checks whether the latch’s counter has changed 
from when it checked the latch.



You will build a thread-safe B+tree.
→ Page Layout
→ Data Structure
→ STL Iterator
→ Latch Crabbing


计划执行， 数据流从叶到根。 
QUERY PLAN
The operators are arranged in a tree.
Data flows from the leaves of the tree 
up towards the root.
The output of the root node is the 
result of the query

为什么需要排序   order by , distinct, group by 的时候会用到，有利于索引 。 
WHY DO WE NEED TO SORT ?
Queries may request that tuples are sorted in a 
specific way (ORDER BY).
But even if a query does not specify an order, we 
may still want to sort to do other things:
→ Trivial to support duplicate elimination (DISTINCT).
→ Bulk loading sorted tuples into a B+Tree index is faster.
→ Aggregations (GROUP BY)


‘
因为受读写磁盘效率的影响， 快速排序 不太适用。 
SORTING ALGORITHMS
If data fits in memory, then we can use a standard 
sorting algorithm like quick-sort. 
If data does not fit in memory, then we need to use 
a technique that is aware of the cost of reading and 
writing from the disk in pages…


合并排序
EXTERNAL MERGE SORT
Divide-and-conquer algorithm that splits the data 
set into separate runs, sorts them individually, and 
then combine into larger sorted runs.
Phase #1 – Sorting
→ Sort blocks of data that fit in main-memory and then 
write back the sorted blocks to a file on disk.
Phase #2 – Merging
→ Combine sorted sub-files into a single larger file

sorted run 排序流程
A run is a list of key/value pairs.
Key: The attribute(s) to compare
to compute the sort order.
Value: Two choices
→ Record Id (late materialization).
→ Tuple (early materialization).

两路合并
2-WAY EXTERNAL MERGE SORT
We will start with a simple example of a 2-way 
external merge sort.
→ "2" represents the number of runs that we are going to 
merge into a new run for each pass.
Data set is broken up into N pages.
The DBMS has a finite number of B buffer pages 
to hold input and output data.
两路外部全并排序 
2-WAY EXTERNAL MERGE SORT
Pass #0
→ Read every B pages of the table into memory
→ Sort pages into runs and write them back to disk.
Pass #1,2,3,…
→ Recursively merges pairs of runs into runs twice as long.
→ Uses three buffer pages (2 for input pages, 1 for output).

受磁盘影响， 可能多个buffer也没用
2-WAY EXTERNAL MERGE SORT
This algorithm only requires three buffer pages to 
perform the sorting (B=3).
→ Two Input Pages, One Output Page
But even if we have more buffer space available 
(B>3), it does not effectively utilize them if the 
worker must block on disk I/O…

摊派
DOUBLE BUFFERING OPTIMIZATION
Prefetch the next run in the background and store 
it in a second buffer while the system is processing 
the current run.
→ Reduces the wait time for I/O requests at each step by 
continuously utilizing the disk.

在B树上做排序
簇与非簇
USING B+TREES FOR SORTING
If the table that must be sorted already has a 
B+Tree index on the sort attribute(s), then we can 
use that to accelerate sorting.
Retrieve tuples in desired sort order by simply 
traversing the leaf pages of the tree.
Cases to consider:
→ Clustered B+Tree
→ Unclustered B+Tree


如何磁盘顺序是数据排序顺序
This is always better than external 
sorting because there is no 
computational cost, and all disk access 
is sequential.


如果磁盘顺序和索引顺序不一致， 不利于IO。


group by , distinct不需要排序的时候， 用hash比较好。 
What if we do not need the data to be ordered?
→ Forming groups in GROUP BY (no ordering)
→ Removing duplicates in DISTINCT (no ordering)
Hashing is a better alternative in this scenario.
→ Only need to remove duplicates, no need for ordering.
→ Can be computationally cheaper than sorting.


hash索引 如何做group, 运算，分到不同frame， 算结果 
PHASE #1 PARTITION
Use a hash function h1
to split tuples into 
partitions on disk.
→ A partition is one or more pages that contain the set of 
keys with the same hash value. 
→ Partitions are "spilled" to disk via output buffers.
Assume that we have B buffers.
We will use B-1 buffers for the partitions and 1
buffer for the input data
如 distinct不要求排序 的时候， 用hash比较好。 
PHASE #2 REHASH
For each partition on disk:
→ Read it into memory and build an in-memory hash table 
based on a second hash function h2
.
→ Then go through each bucket of this hash table to bring 
together matching tuples.
This assumes that each partition fits in memory.

为什么需要Join, 正规化是把重复的数据分开。 出结果的时候再通过Join把数据关联起来。 
WHY DO WE NEED TO JOIN?
We normalize tables in a relational database to 
avoid unnecessary repetition of information.
We use the join operate to reconstruct the original 
tuples without any information loss.