three main types of failures:
» Disk (storage media) failure
» System crash
» Transaction failure


Database Management Systems, 3rd Edition
by Raghu Ramakrishnan and Johannes Gehrke
The following book is required for the class. A few lectures will review material from this textbook and will include readings from this book. Please make sure to get the third edition of this book.

Database Management Systems, 3rd Edition. Raghu Ramakrishnan and Johannes Gehrke.
The following are additional books that you may find useful:

Foundations of Databases (Abiteboul, Hull & Vianu)
Database Systems: the complete book (Ullman, Widom and Garcia-Molina)
Parallel and Distributed DBMS (Ozsu and Valduriez)
Transaction Processing (Gray and Reuter)
Data and Knowledge based Systems (volumes I, II) (Ullman)
Data on the Web (Abiteboul, Buneman, Suciu)
Readings in Database Systems (4th and 3rd ed) (Stonebraker and Hellerstein)
Proceedings of SIGMOD, VLDB, ICDE, and CIDR conferences.

### http://www.scs.stanford.edu/20sp-cs244b/notes/  distributed system
### http://www.scs.stanford.edu/20sp-cs244b/notes/
## cs 149  PARALLEL COMPUTING 
## cs 149  PARALLEL COMPUTING
## cs 149  PARALLEL COMPUTING

Isolation levels:
» Level 1: Transaction may read uncommitted data; 
successive reads to a record may return different values
» Level 2: Transaction may only read committed data, but 
successive reads can differ
» Level 3: Successive reads return same value

Locking:
» Started with “predicate locks” based on expressions: 
too expensive
» Moved to hierarchical locks: record/page/table, with 
read/write types and intentions



Transactional DBMS: focus on concurrent, 
small, low-latency transactions (e.g. MySQL, 
Postgres, Oracle, DB2) → real-time apps
Analytical DBMS: focus on large, parallel 
but mostly read-only analytics (e.g. Teradata, 
Redshift, Vertica) → “data warehouses


Fixed Format
A schema for all records in table specifies:
- # of fields
- type of each field
- order in record
- meaning of each field


Placing Data for Efficient 
Access
Locality: which items are accessed together
» When you read one field of a record, you’re 
likely to read other fields of the same record
» When you read one field of record 1, you’re 
likely to read the same field of record 2
Searchability: quickly find relevant records
» E.g. sorting the file lets you do binary search



Can We Have Hybrids 
Between Row & Column?
Yes! For example, colocated column groups: 


Improving Searchability: 
Ordering
Ordering the data by a field will give:
» Closer I/Os if queries tend to read data with 
nearby values of the field (e.g. time ranges)
» Option to accelerate search via an ordered 
index (e.g. B-tree), binary search, etc


Questions in Storing Records
(1) separating records
(2) spanned vs. unspanned
(3) indirection


C-Store


Handling Duplicate Keys
For a primary index, can point to 1st instance 
of each item (assuming blocks are linked)
For a secondary index, need to point to a list 
of records since they can be anywhere


k-d tree m dimension index
Splits dimensions in any order 
to hold k-dimensional data


Wide range of indexes for different data 
types and queries (e.g. range vs exact)
Key concerns: query time, cost to update, 
and size of index


Execution Methods: Once We 
Have a Plan, How to Run it?
Several options that trade between 
complexity, performance and startup time


Outline
What can we optimize?
Rule-based optimization
Data statistics
Cost models
Cost-based plan selection


Join algorithms can have different 
performance in different situations
In general, the following are used:
» Index join if an index exists
» Merge join if at least one table is sorted
» Hash join if both tables unsorted


SQL standard defines serializability as “same 
as a serial schedule”, but then also lists 3 
types of “anomalies” to define levels:


There are isolation levels other than 
serializability that meet the last definition!
» I.e. don’t exhibit those 3 anomalies
Virtually no commercial DBs do serializability 
by default, and some can’t do it at all
Time to call the lawyers?


Enforcing serializability via 2-phase locking
» Shared and exclusive locks
» Lock tables and multi-level locking


Want schedules that are “good”, regardless of
» initial state and
» transaction semantics


------------------------------
Transactions
Concurrency
Transactions
Concurrency
Transactions
Concurrency
------------------------------

the eight fallacies of distributed computing
the eight fallacies of distributed computing
the eight fallacies of distributed computing


Replication
Store each data item on multiple nodes!



Traditional DB: page the DBA
Distributed computing: use consensus
» Several algorithms: Paxos, Raft
» Today: many implementations
• Apache Zookeeper, etcd, Consul
» Idea: keep a reliable, distributed shared 
record of who is “primary”


Two Phase Commit (2PC)
1. Transaction coordinator sends prepare
message to each participating node
2. Each participating node responds to 
coordinator with prepared or no
3. If coordinator receives all prepared:
» Broadcast commit
4. If coordinator receives any no:
» Broadcast abort


Traditionally: run 2PC at commit time
» i.e., perform locking as usual, then run 2PC 
to have all participants agree that the 
transaction will commit
Under strict 2PL, run 2PC before unlocking 
the write locks

Two-phase locking

2PL: piggyback lock “unlock” commands on 
commit/abort message

Case 2: Coordinator 
Unavailable
Participants cannot make progress
But: can agree to elect a new coordinator, 
never listen to the old one (using consensus)
» Old coordinator comes back? Overruled by 
participants, who reject its messages


CAP Theorem
In an asynchronous network, a distributed 
database can either:
» guarantee a response from any replica in a 
finite amount of time (“availability”) OR
» guarantee arbitrary “consistency” 
criteria/constraints about data
but not both


“CAP” is a reminder:
» No free lunch for distributed systems


CAP Theorem
Choose either:
» Consistency and “Partition tolerance” (CP)
» Availability and “Partition tolerance” (AP)
Example consistency criteria:
» Exactly one key can have value “Matei”
CAP is a reminder: no free lunch for 
distributed systems


Let’s Talk About Coordination
If we’re “AP”, then we don’t have to talk 
even when we can!
If we’re “CP”, then we have to talk all the 
time
How fast can we send messages


Punchlines:
Serializability has a provable cost to latency, 
availability, scalability (if there are conflicts)
We can avoid this penalty if we are willing to 
look at our application and our application 
does not require coordination
» Major topic of ongoing research



coordination avoidance in database systems
coordination avoidance in database systems


lessons from internet services: ACID vs. BASE
lessons from internet services: ACID vs. BASE


Avoiding Coordination
Key techniques for BASE:
» Partition data so that most transactions are 
local to one partition
» Tolerate stale data (eventual consistency):
• Caches
• Weaker isolation levels
• Helpful ideas: idempotence, commutativity



BASE EXAMPLE 
Constraint: each 
user’s amt_sold and 
amt_bought is sum of 
their transactions
ACID Approach: to add a transaction, use 2PC to 
update transactions table + records for buyer, seller
One BASE approach: write new transactions to the 
transactions table and use a periodic batch job to fill 
in the users table


Helpful Ideas
When we delay applying updates to an item, 
must ensure we only apply each update once
» Issue if we crash while applying!
» Idempotent operations: same result if you 
apply them twice
When different nodes want to update multiple 
items, want result independent of msg order
» Commutative operations: A⍟B = B⍟A


Amdahl’s Law
If p is the fraction of the program that can be 
made parallel, running time with N nodes is
T(n) = 1 - p + p/N
Result: max possible speedup is 1 / (1 - p)
Example: 80% parallelizable ⇒ 5x speedup


Example System Designs
Traditional “massively parallel” DBMS
» Tables partitioned evenly across nodes
» Each physical operator also partitioned
» Pipelining across these operators
MapReduce
» Focus on unreliable, commodity nodes
» Divide work into idempotent tasks, and use 
dynamic algorithms for load balancing, fault 
recovery and straggler recovery


Example: Distributed Joins
Say we want to compute A ⨝ B, where A 
and B are both partitioned across N nodes:


Example: Distributed Joins
Say we want to compute A ⨝ B, where A 
and B are both partitioned across N nodes
Algorithm 1: shuffle hash join
» Each node hashes records of A, B to N 
partitions by key, sends partition i to node i
» Each node then joins the records it received
Communication cost: (N-1)/N (|A| + |B|)


Algorithm 2: broadcast join on B
» Each node broadcasts its partition of B to all 
other nodes
» Each node then joins B against its A partition

Takeaway
Broadcast join is much faster if |B| ≪ |A|


Handling Faults & Stragglers
If uncommon, just ignore / call the operator / 
restart query
Problem: probability of something bad 
grows fast with number of nodes
» E.g. if one node has 0.1% probability of 
straggling, then with 1000 nodes,
P(none straggles) = (1 - 0.001)1000 ≈ 0.3


Straggler Recovery Methods
General idea: send the slow request/task to 
another node (launch a “backup task”)
Threshold approach: if a task is slower than 
99th percentile, or 1.5x avg, etc, launch backup
Progress-based approach:
estimate task finish times and launch
tasks likeliest to finish last


What is Cloud Computing?
Computing as a service, managed by an 
external party
» Software as a Service (SaaS): application 
hosted by a provider, e.g. Salesforce, Gmail
» Platform as a Service (PaaS): APIs to 
program against, e.g. DB or web hosting
» Infrastructure as a Service (IaaS): raw 
computing resources, e.g. VMs on AW


Dynamo Implementation
Commodity nodes with local 
storage on disks
Nodes form a “ring” to split up 
the key space among them
» Actually, each node covers 
many ranges (over-partitioning)
Use quorums and gossip to 
manage updates to each key


Just run an existing DBMS (e.g. MySQL) on 
cloud VMs, and use replicated disk storage


Aurora’s Design
Implement replication at a higher level: only 
replicate the redo log (not disk blocks)
Enable elastic frontend and backend by 
decoupling API & storage servers
» Lower cost & higher performance per tenant


Design Details
Logging uses async quorum: wait until 4 of 6 
nodes reply (faster than waiting for all 6)
Each storage node takes
the log and rebuilds the
DB pages locally
Care taken to handle
incomplete logs due
to async quorums

Delta Lake Motivation


Delta Lake Motivation
Object stores are the largest & lowest-cost 
storage systems, but their semantics make it 
hard to manage mutable datasets
Goal: analytical table storage over object 
stores, built as a client library (no other services)
Interface: relational tables with SQL queries
Consistency: serializable ACID transactions


CQL = Continuous Query Language; 
research project by our dean Jennifer 
Widom


More recent API, used at Google and open 
sourced (API only) as Apache Beam


Heap File Implementation 


定长记录， 插入中间， 得移动顺序？


Clustered/Unclustered 
• Primary index = clustered by definition 
• Secondary indexes = usually unclustered 


R tree index空间划分树

Canonical database search tree 
• Balanced tree with high fanout
• Leaf nodes contain pointers to actual data 
• Leaf nodes stored as a linked list 
• Internal nodes used as a directory 
– Contain <key,pointers> pairs 
– If key consistent with query, data may be found if we follow pointer 
– Generalized search key: predicate that holds for each entry below key 
• B+-tree key is pair of integers <a,b> and predicate is Contains([a,b),v) 
• R-tree key is bounding box and predicate is also containment test 
– Generalized search tree: hierarchy of partitions 



• Union
– Once a key is inserted, need to adjust predicates at parent nodes 
– See algorithm AdjustKeys(R,N) 
– B+-tree: computes interval that covers all given intervals 
– R-tree: computes bigger bounding box 


• Step 3: Query optimization
– Find an efficient query plan for executing the query 
– We will spend a whole lecture on this topic 
• A query plan is
– Logical query plan: an extended relational algebra tree 
– Physical query plan: with additional annotations at each node 
• Access method to use for each relation 
• Implementation to use for each relational operator 


Extended Algebra Operators 
• Union ∪, intersection ∩, difference - 
• Selection σ
• Projection π
• Join 
• Duplicate elimination δ
• Grouping and aggregation γ
• Sorting τ
• Rename ρ


• Most optimizers operate on individual query blocks 
• A query block is an SQL query with no nesting 
– Exactly one
• SELECT clause 
• FROM clause
– At most one
• WHERE clause 
• GROUP BY clause 
• HAVING clause


• Logical query plan with extra annotations 
• Access path selection for each relation 
– Use a file scan or use an index 
• Implementation choice for each operator 
• Scheduling decisions for operators


Why Learn About Op Algos? 
• Implemented in commercial DBMSs
• Different DBMSs implement different subsets of these 
algorithms 
• Good algorithms can greatly improve performance 
• Need to know about physical operators to understand 
query optimization


Cost Parameters 
• In database systems the data is on disk 
• Cost = total number of I/Os
• Parameters: 
– B(R) = # of blocks (i.e., pages) for relation R 
– T(R) = # of tuples in relation R 
– V(R, a) = # of distinct values of attribute a 


 Cost of an operation = number of disk I/Os to 
– read the operands 
– compute the result 
• Cost of writing the result to disk is not included
– Need to count it separately when applicable

• Clustered relation R: 
– Blocks consists mostly of records from this table 
– B(R) ≈ T(R) / blockSize 
• Unclustered relation R: 
– Its records are placed on blocks with other tables 
– When R is unclustered: B(R) ≈ T(R) 


Clustered relation: 
– Result may be unsorted: B(R) 
– Result needs to be sorted: 3B(R) 
• Unclustered relation 
– Unsorted: T(R) 
– Sorted: T(R) + 2B(R)


Join Algorithms 
• Logical operator: 
– Product(pname, cname) ⋈ Company(cname, city) 
• Propose three physical operators for the join, assuming 
the tables are in main memory: 
– Hash join 
– Nested loop join
– Sort-merge join


Hash Join 
Hash join: R ⋈ S 
• Scan R, build buckets in main memory 
• Then scan S and join 
• Cost: B(R) + B(S) 
• One pass algorithm when B(R) <= M （memory?）


Nested Loop Joins 
• Tuple-based nested loop R ⋈ S 
• R is the outer relation, S is the inner relation 
• Cost: B(R) + T(R) B(S) when S is clustered 
• Cost: B(R) + T(R) T(S) when S is unclustered 
for each tuple r in R do 
 for each tuple s in S do 
	 if r and s join then output (r,s) 


• Steps involved in processing a query 
– Logical query plan 
– Physical query plan 
– Query execution overview 
• Operator implementations 
– One pass algorithms 
– Two-pass algorithms 
– Index-based algorithms 


Two-Pass Algorithms 
• What if data does not fit in memory? 
• Need to process it in multiple passes 
• Two key techniques 
– Hashing 
– Sorting


Query Optimization Algorithm
• For a query 
– There exists many physical query plans 
– Query optimizer needs to pick a good one 
• Basic query optimization algorithm 
– Enumerate alternative plans 
– Compute estimated cost of each plan
• Compute number of I/Os 
• Optionally take into account other resources 
– Choose plan with lowest cost 
– This is called cost-based optimization



Access path selectivity is the number of pages 
retrieved if we use this access path
– Most selective retrieves fewest pages 
• As we saw earlier, for equality predicates
– Selection on equality: σa=v(R) 
– V(R, a) = # of distinct values of attribute a 
– 1/V(R,a) is thus the reduction factor 
– Clustered index on a: cost B(R)/V(R,a) 
– Unclustered index on a: cost T(R)/V(R,a) 
– (we are ignoring I/O cost of index pages for simplicity)



Selection on range: σa>v(R) 
• How to compute the selectivity?
• Assume values are uniformly distributed
• Reduction factor X 
• X = (Max(R,a) - v) / (Max(R,a) - Min(R,a)) 
• Clustered index on a: cost B(R)*X 
• Unclustered index on a: cost T(R)*X


• Selection condition: sid > 300 ∧ scity=‘Seattle’ 
– Index I1: B+-tree on sid clustered 
– Index I2: B+-tree on scity unclustered 
• Let’s assume 
– V(Supplier,scity) = 20 
– Max(Supplier, sid) = 1000, Min(Supplier,sid)=1 
– B(Supplier) = 100, T(Supplier) = 1000 
• Cost I1: B(R) * (Max-v)/(Max-Min) = 100*700/999 ≈ 70 
• Cost I2: T(R) * 1/V(Supplier,scity) = 1000/20 = 50 


• For each operator compute
– Estimate cost of executing the operation
– Estimate statistical summary of the output data


Relational Algebra Equivalences 
• Selects, projects, and joins 
– We can commute and combine all three types of operators 
– We just have to be careful that the fields we need are available 
when we apply the operator
– Relatively straightforward. See book 15.3. 
• If you like this topic, more info in optional paper (by 
Chaudhuri), Section 4. 


Plan Enumeration Algorithm 
• Idea: use dynamic programming
• For each subset of {R1, …, Rn}, compute the best plan 
for that subset 
• In increasing order of set cardinality: 
– Step 1: for {R1}, {R2}, …, {Rn} 
– Step 2: for {R1,R2}, {R1,R3}, …, {Rn-1, Rn} 
– … 
– Step n: for {R1, …, Rn} 
• It is a bottom-up strategy 
• A subset of {R1, …, Rn} is also called a subquery 


 Turing awards to database researchers: 
– Charles Bachman 1973 
– Edgar Codd 1981 for inventing relational dbs
– Jim Gray 1998 for inventing transactions 


ACID Properties 
• Atomicity: Either all changes performed by transaction 
occur or none occurs 
• Consistency: A transaction as a whole does not violate 
integrity constraints 
• Isolation: Transactions appear to execute one after the 
other in sequence 
• Durability: If a transaction commits, its changes will survive 
failures


Why is it hard to provide ACID properties? 
• Concurrent operations 
– Isolation problems 
– We saw one example earlier 
• Failures can occur at any time 
– Atomicity and durability problems 
– Next lecture 
• Transaction may need to abort


Types of Problems: Summary 
• Concurrent execution problems
– Write-read conflict: dirty read
• A transaction reads a value written by another transaction that has 
not yet committed 
– Read-write conflict: unrepeatable read
• A transaction reads the value of the same object twice. Another 
transaction modifies that value in between the two reads 
– Write-write conflict: lost update
• Two transactions update the value of the same object. The second 
one to write the value overwrite the first change 



Deadlocks 
• Two or more transactions are waiting for each other to 
complete 
• Deadlock avoidance
– Acquire locks in pre-defined order 
– Acquire all locks at once before starting 
• Deadlock detection
– Timeouts 
– Wait-for graph 
• This is what commercial systems use (they check graph periodically


Degrees of Isolation 
• Isolation level “serializable” (i.e. ACID) 
– Golden standard 
– Requires strict 2PL and predicate locking 
– But often too inefficient 
– Imagine there are only a few update operations and many long 
read operations 
• Weaker isolation levels 
– Sacrifice correctness for efficiency 
– Often used in practice (often default) 
– Sometimes are hard to understand 


Four levels of isolation
– All levels use long-duration exclusive locks
– READ UNCOMMITTED: no read locks 
– READ COMMITTED: short duration read locks 
– REPEATABLE READ: 
• Long duration read locks on individual items 
– SERIALIZABLE: 
• All locks long duration and lock predicates


Lock Granularity 
• Fine granularity locking (e.g., tuples) 
– High concurrency 
– High overhead in managing locks 
• Coarse grain locking (e.g., tables) 
– Many false conflicts 
– Less overhead in managing locks 
• Alternative techniques 
– Hierarchical locking (and intentional locks) [commercial DBMSs] 
– Lock escalation 


The Tree Protocol 
Rules: 
• A lock on a node A may only be acquired if the transaction holds a 
lock on its parent B 
• Nodes can be unlocked in any order (no 2PL necessary) 
• Cannot relock a node for which already released a lock 
• “Crabbing” 
– First lock parent then lock child 
– Keep parent locked only if may need to update it 
– Release lock on parent if child is not full
• The tree protocol is NOT 2PL, yet ensures conflict-serializability !


Optimistic Concurrency Control 
Validation-based technique 
• Phase 1: Read
– Transaction reads from database and writes to a private workspace 
• Phase 2: Validate 
– At commit time, system performs validation 
– Validation checks if transaction could have conflicted with others 
• Each transaction gets a timestamp 
• Check if timestamp order is equivalent to a serial order 
– If there is a potential conflict: abort 
• Phase 3: Write 
– If no conflict, transaction changes are copied into database


Optimistic Concurrency Control 
Timestamp-based technique 
• Each object, O, has read and write timestamps: RTS(O) and WTS(O) 
• Each transaction, T, has a timestamp TS(T) 
• Transaction wants to read object O
– If TS(T) < WTS(O) abort 
– Else read and update RTS(O) to larger of TS(T) or RTS(O) 
• Transaction wants to write object O
– If TS(T) < RTS(O) abort 
– If TS(T) < WTS(O) ignore my write and continue (Thomas Write Rule) 
– Otherwise, write O and update WTS(O) to TS(T) 


Multiversion-based technique 
• Object timestamps: RTS(O) & WTS(O); transaction timestamps TS(T) 
• Transaction can read most recent version that precedes TS(T) 
– When reading object, update RTS(O) to larger of TS(T) or RTS(O) 
• Transaction wants to write object O
– If TS(T) < RTS(O) abort 
– Otherwise, create a new version of O with WTS(O) = TS(T) 
• Common variant (used in commercial systems) 
– To write object O only check for conflicting writes not reads 
– Use locks for writes to avoid aborting in case conflicting transaction aborts


Buffer Manager Policies 
• STEAL or NO-STEAL
– Can an update made by an uncommitted transaction overwrite the most 
recent committed value of a data item on disk? 
• FORCE or NO-FORCE
– Should all updates of a transaction be forced to disk before the 
transaction commits? 
• Easiest for recovery: NO-STEAL/FORCE 
• Highest performance: STEAL/NO-FORCE 


Solution: Use a Log 
• Log: append-only file containing log records
• Enables the use of STEAL and NO-FORCE 
• For every update, commit, or abort operation 
– Write physical, logical, or physiological log record 
– Note: multiple transactions run concurrently, log records are 
interleaved 
• After a system crash, use log to: 
– Redo some transaction that did commit 
– Undo other transactions that didn’t commit 


Write-Ahead Log 
• All log records pertaining to a page are written to disk 
before the page is overwritten on disk 
• All log records for transaction are written to disk before
the transaction is considered committed 
– Why is this faster than FORCE policy? 
• Committed transaction: transactions whose commit log 
record has been written to disk


ARIES Method 
• Write-Ahead Log 
• Three pass algorithm 
– Analysis pass 
• Figure out what was going on at time of crash 
• List of dirty pages and active transactions 
– Redo pass (repeating history principle)
• Redo all operations, even for transactions that will not commit 
• Get back to state at the moment of the crash 
– Undo pass
• Remove effects of all uncommitted transactions 
• Log changes during undo in case of another crash during undo


log sequence number (LSN)

performance metrics for parallel dbms
• Speedup 
– More processors  higher speed
• Scalup
– More processors  can process more data 
– Transaction scaleup vs batch scaleup
• Challenges to speedup and scalup
– Startup cost: cost of starting an operation on many processors 
– Interference: contention for resources between processors 
– Skew: slowest step becomes the bottleneck


Architectures for Parallel Databases 
• Shared memory 
• Shared disk 
• Shared nothing 


Taxonomy for 
Parallel Query Evaluation 
• Inter-query parallelism 
– Each query runs on one processor 
• Inter-operator parallelism 
– A query runs on multiple processors 
– An operator runs on one processor 
• Intra-operator parallelism 
– An operator runs on multiple processors 
13 
We study only intra-operator parallelism: most scalable