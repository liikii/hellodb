three main types of failures:
» Disk (storage media) failure
» System crash
» Transaction failure


princeton cs 145 intro to dbs.

Database Management Systems, 3rd Edition
by Raghu Ramakrishnan and Johannes Gehrke
The following book is required for the class. A few lectures will review material from this textbook and will include readings from this book. Please make sure to get the third edition of this book.

Database Management Systems, 3rd Edition. Raghu Ramakrishnan and Johannes Gehrke.
The following are additional books that you may find useful:

Foundations of Databases (Abiteboul, Hull & Vianu)
Database Systems: the complete book (Ullman, Widom and Garcia-Molina)
Parallel and Distributed DBMS (Ozsu and Valduriez)
Transaction Processing (Gray and Reuter)
Data and Knowledge based Systems (volumes I, II) (Ullman)
Data on the Web (Abiteboul, Buneman, Suciu)
Readings in Database Systems (4th and 3rd ed) (Stonebraker and Hellerstein)
Proceedings of SIGMOD, VLDB, ICDE, and CIDR conferences.

### http://www.scs.stanford.edu/20sp-cs244b/notes/  distributed system
### http://www.scs.stanford.edu/20sp-cs244b/notes/
## cs 149  PARALLEL COMPUTING 
## cs 149  PARALLEL COMPUTING
## cs 149  PARALLEL COMPUTING

Isolation levels:
» Level 1: Transaction may read uncommitted data; 
successive reads to a record may return different values
» Level 2: Transaction may only read committed data, but 
successive reads can differ
» Level 3: Successive reads return same value

Locking:
» Started with “predicate locks” based on expressions: 
too expensive
» Moved to hierarchical locks: record/page/table, with 
read/write types and intentions



Transactional DBMS: focus on concurrent, 
small, low-latency transactions (e.g. MySQL, 
Postgres, Oracle, DB2) → real-time apps
Analytical DBMS: focus on large, parallel 
but mostly read-only analytics (e.g. Teradata, 
Redshift, Vertica) → “data warehouses


Fixed Format
A schema for all records in table specifies:
- # of fields
- type of each field
- order in record
- meaning of each field


Placing Data for Efficient 
Access
Locality: which items are accessed together
» When you read one field of a record, you’re 
likely to read other fields of the same record
» When you read one field of record 1, you’re 
likely to read the same field of record 2
Searchability: quickly find relevant records
» E.g. sorting the file lets you do binary search



Can We Have Hybrids 
Between Row & Column?
Yes! For example, colocated column groups: 


Improving Searchability: 
Ordering
Ordering the data by a field will give:
» Closer I/Os if queries tend to read data with 
nearby values of the field (e.g. time ranges)
» Option to accelerate search via an ordered 
index (e.g. B-tree), binary search, etc


Questions in Storing Records
(1) separating records
(2) spanned vs. unspanned
(3) indirection


C-Store


Handling Duplicate Keys
For a primary index, can point to 1st instance 
of each item (assuming blocks are linked)
For a secondary index, need to point to a list 
of records since they can be anywhere


k-d tree m dimension index
Splits dimensions in any order 
to hold k-dimensional data


Wide range of indexes for different data 
types and queries (e.g. range vs exact)
Key concerns: query time, cost to update, 
and size of index


Execution Methods: Once We 
Have a Plan, How to Run it?
Several options that trade between 
complexity, performance and startup time


Outline
What can we optimize?
Rule-based optimization
Data statistics
Cost models
Cost-based plan selection


Join algorithms can have different 
performance in different situations
In general, the following are used:
» Index join if an index exists
» Merge join if at least one table is sorted
» Hash join if both tables unsorted


SQL standard defines serializability as “same 
as a serial schedule”, but then also lists 3 
types of “anomalies” to define levels:


There are isolation levels other than 
serializability that meet the last definition!
» I.e. don’t exhibit those 3 anomalies
Virtually no commercial DBs do serializability 
by default, and some can’t do it at all
Time to call the lawyers?


Enforcing serializability via 2-phase locking
» Shared and exclusive locks
» Lock tables and multi-level locking


Want schedules that are “good”, regardless of
» initial state and
» transaction semantics


------------------------------
Transactions
Concurrency
Transactions
Concurrency
Transactions
Concurrency
------------------------------

the eight fallacies of distributed computing
the eight fallacies of distributed computing
the eight fallacies of distributed computing


Replication
Store each data item on multiple nodes!



Traditional DB: page the DBA
Distributed computing: use consensus
» Several algorithms: Paxos, Raft
» Today: many implementations
• Apache Zookeeper, etcd, Consul
» Idea: keep a reliable, distributed shared 
record of who is “primary”


Two Phase Commit (2PC)
1. Transaction coordinator sends prepare
message to each participating node
2. Each participating node responds to 
coordinator with prepared or no
3. If coordinator receives all prepared:
» Broadcast commit
4. If coordinator receives any no:
» Broadcast abort


Traditionally: run 2PC at commit time
» i.e., perform locking as usual, then run 2PC 
to have all participants agree that the 
transaction will commit
Under strict 2PL, run 2PC before unlocking 
the write locks

Two-phase locking

2PL: piggyback lock “unlock” commands on 
commit/abort message

Case 2: Coordinator 
Unavailable
Participants cannot make progress
But: can agree to elect a new coordinator, 
never listen to the old one (using consensus)
» Old coordinator comes back? Overruled by 
participants, who reject its messages


CAP Theorem
In an asynchronous network, a distributed 
database can either:
» guarantee a response from any replica in a 
finite amount of time (“availability”) OR
» guarantee arbitrary “consistency” 
criteria/constraints about data
but not both


“CAP” is a reminder:
» No free lunch for distributed systems


CAP Theorem
Choose either:
» Consistency and “Partition tolerance” (CP)
» Availability and “Partition tolerance” (AP)
Example consistency criteria:
» Exactly one key can have value “Matei”
CAP is a reminder: no free lunch for 
distributed systems


Let’s Talk About Coordination
If we’re “AP”, then we don’t have to talk 
even when we can!
If we’re “CP”, then we have to talk all the 
time
How fast can we send messages


Punchlines:
Serializability has a provable cost to latency, 
availability, scalability (if there are conflicts)
We can avoid this penalty if we are willing to 
look at our application and our application 
does not require coordination
» Major topic of ongoing research



coordination avoidance in database systems
coordination avoidance in database systems


lessons from internet services: ACID vs. BASE
lessons from internet services: ACID vs. BASE


Avoiding Coordination
Key techniques for BASE:
» Partition data so that most transactions are 
local to one partition
» Tolerate stale data (eventual consistency):
• Caches
• Weaker isolation levels
• Helpful ideas: idempotence, commutativity



BASE EXAMPLE 
Constraint: each 
user’s amt_sold and 
amt_bought is sum of 
their transactions
ACID Approach: to add a transaction, use 2PC to 
update transactions table + records for buyer, seller
One BASE approach: write new transactions to the 
transactions table and use a periodic batch job to fill 
in the users table


Helpful Ideas
When we delay applying updates to an item, 
must ensure we only apply each update once
» Issue if we crash while applying!
» Idempotent operations: same result if you 
apply them twice
When different nodes want to update multiple 
items, want result independent of msg order
» Commutative operations: A⍟B = B⍟A


Amdahl’s Law
If p is the fraction of the program that can be 
made parallel, running time with N nodes is
T(n) = 1 - p + p/N
Result: max possible speedup is 1 / (1 - p)
Example: 80% parallelizable ⇒ 5x speedup


Example System Designs
Traditional “massively parallel” DBMS
» Tables partitioned evenly across nodes
» Each physical operator also partitioned
» Pipelining across these operators
MapReduce
» Focus on unreliable, commodity nodes
» Divide work into idempotent tasks, and use 
dynamic algorithms for load balancing, fault 
recovery and straggler recovery


Example: Distributed Joins
Say we want to compute A ⨝ B, where A 
and B are both partitioned across N nodes:


Example: Distributed Joins
Say we want to compute A ⨝ B, where A 
and B are both partitioned across N nodes
Algorithm 1: shuffle hash join
» Each node hashes records of A, B to N 
partitions by key, sends partition i to node i
» Each node then joins the records it received
Communication cost: (N-1)/N (|A| + |B|)


Algorithm 2: broadcast join on B
» Each node broadcasts its partition of B to all 
other nodes
» Each node then joins B against its A partition

Takeaway
Broadcast join is much faster if |B| ≪ |A|


Handling Faults & Stragglers
If uncommon, just ignore / call the operator / 
restart query
Problem: probability of something bad 
grows fast with number of nodes
» E.g. if one node has 0.1% probability of 
straggling, then with 1000 nodes,
P(none straggles) = (1 - 0.001)1000 ≈ 0.3


Straggler Recovery Methods
General idea: send the slow request/task to 
another node (launch a “backup task”)
Threshold approach: if a task is slower than 
99th percentile, or 1.5x avg, etc, launch backup
Progress-based approach:
estimate task finish times and launch
tasks likeliest to finish last


What is Cloud Computing?
Computing as a service, managed by an 
external party
» Software as a Service (SaaS): application 
hosted by a provider, e.g. Salesforce, Gmail
» Platform as a Service (PaaS): APIs to 
program against, e.g. DB or web hosting
» Infrastructure as a Service (IaaS): raw 
computing resources, e.g. VMs on AW


Dynamo Implementation
Commodity nodes with local 
storage on disks
Nodes form a “ring” to split up 
the key space among them
» Actually, each node covers 
many ranges (over-partitioning)
Use quorums and gossip to 
manage updates to each key


Just run an existing DBMS (e.g. MySQL) on 
cloud VMs, and use replicated disk storage


Aurora’s Design
Implement replication at a higher level: only 
replicate the redo log (not disk blocks)
Enable elastic frontend and backend by 
decoupling API & storage servers
» Lower cost & higher performance per tenant


Design Details
Logging uses async quorum: wait until 4 of 6 
nodes reply (faster than waiting for all 6)
Each storage node takes
the log and rebuilds the
DB pages locally
Care taken to handle
incomplete logs due
to async quorums

Delta Lake Motivation


Delta Lake Motivation
Object stores are the largest & lowest-cost 
storage systems, but their semantics make it 
hard to manage mutable datasets
Goal: analytical table storage over object 
stores, built as a client library (no other services)
Interface: relational tables with SQL queries
Consistency: serializable ACID transactions


CQL = Continuous Query Language; 
research project by our dean Jennifer 
Widom


More recent API, used at Google and open 
sourced (API only) as Apache Beam


Heap File Implementation 


定长记录， 插入中间， 得移动顺序？


Clustered/Unclustered 
• Primary index = clustered by definition 
• Secondary indexes = usually unclustered 


R tree index空间划分树

Canonical database search tree 
• Balanced tree with high fanout
• Leaf nodes contain pointers to actual data 
• Leaf nodes stored as a linked list 
• Internal nodes used as a directory 
– Contain <key,pointers> pairs 
– If key consistent with query, data may be found if we follow pointer 
– Generalized search key: predicate that holds for each entry below key 
• B+-tree key is pair of integers <a,b> and predicate is Contains([a,b),v) 
• R-tree key is bounding box and predicate is also containment test 
– Generalized search tree: hierarchy of partitions 



• Union
– Once a key is inserted, need to adjust predicates at parent nodes 
– See algorithm AdjustKeys(R,N) 
– B+-tree: computes interval that covers all given intervals 
– R-tree: computes bigger bounding box 


• Step 3: Query optimization
– Find an efficient query plan for executing the query 
– We will spend a whole lecture on this topic 
• A query plan is
– Logical query plan: an extended relational algebra tree 
– Physical query plan: with additional annotations at each node 
• Access method to use for each relation 
• Implementation to use for each relational operator 


Extended Algebra Operators 
• Union ∪, intersection ∩, difference - 
• Selection σ
• Projection π
• Join 
• Duplicate elimination δ
• Grouping and aggregation γ
• Sorting τ
• Rename ρ


• Most optimizers operate on individual query blocks 
• A query block is an SQL query with no nesting 
– Exactly one
• SELECT clause 
• FROM clause
– At most one
• WHERE clause 
• GROUP BY clause 
• HAVING clause


• Logical query plan with extra annotations 
• Access path selection for each relation 
– Use a file scan or use an index 
• Implementation choice for each operator 
• Scheduling decisions for operators


Why Learn About Op Algos? 
• Implemented in commercial DBMSs
• Different DBMSs implement different subsets of these 
algorithms 
• Good algorithms can greatly improve performance 
• Need to know about physical operators to understand 
query optimization


Cost Parameters 
• In database systems the data is on disk 
• Cost = total number of I/Os
• Parameters: 
– B(R) = # of blocks (i.e., pages) for relation R 
– T(R) = # of tuples in relation R 
– V(R, a) = # of distinct values of attribute a 


 Cost of an operation = number of disk I/Os to 
– read the operands 
– compute the result 
• Cost of writing the result to disk is not included
– Need to count it separately when applicable

• Clustered relation R: 
– Blocks consists mostly of records from this table 
– B(R) ≈ T(R) / blockSize 
• Unclustered relation R: 
– Its records are placed on blocks with other tables 
– When R is unclustered: B(R) ≈ T(R) 


Clustered relation: 
– Result may be unsorted: B(R) 
– Result needs to be sorted: 3B(R) 
• Unclustered relation 
– Unsorted: T(R) 
– Sorted: T(R) + 2B(R)


Join Algorithms 
• Logical operator: 
– Product(pname, cname) ⋈ Company(cname, city) 
• Propose three physical operators for the join, assuming 
the tables are in main memory: 
– Hash join 
– Nested loop join
– Sort-merge join


Hash Join 
Hash join: R ⋈ S 
• Scan R, build buckets in main memory 
• Then scan S and join 
• Cost: B(R) + B(S) 
• One pass algorithm when B(R) <= M （memory?）


Nested Loop Joins 
• Tuple-based nested loop R ⋈ S 
• R is the outer relation, S is the inner relation 
• Cost: B(R) + T(R) B(S) when S is clustered 
• Cost: B(R) + T(R) T(S) when S is unclustered 
for each tuple r in R do 
 for each tuple s in S do 
	 if r and s join then output (r,s) 


• Steps involved in processing a query 
– Logical query plan 
– Physical query plan 
– Query execution overview 
• Operator implementations 
– One pass algorithms 
– Two-pass algorithms 
– Index-based algorithms 


Two-Pass Algorithms 
• What if data does not fit in memory? 
• Need to process it in multiple passes 
• Two key techniques 
– Hashing 
– Sorting


Query Optimization Algorithm
• For a query 
– There exists many physical query plans 
– Query optimizer needs to pick a good one 
• Basic query optimization algorithm 
– Enumerate alternative plans 
– Compute estimated cost of each plan
• Compute number of I/Os 
• Optionally take into account other resources 
– Choose plan with lowest cost 
– This is called cost-based optimization



Access path selectivity is the number of pages 
retrieved if we use this access path
– Most selective retrieves fewest pages 
• As we saw earlier, for equality predicates
– Selection on equality: σa=v(R) 
– V(R, a) = # of distinct values of attribute a 
– 1/V(R,a) is thus the reduction factor 
– Clustered index on a: cost B(R)/V(R,a) 
– Unclustered index on a: cost T(R)/V(R,a) 
– (we are ignoring I/O cost of index pages for simplicity)



Selection on range: σa>v(R) 
• How to compute the selectivity?
• Assume values are uniformly distributed
• Reduction factor X 
• X = (Max(R,a) - v) / (Max(R,a) - Min(R,a)) 
• Clustered index on a: cost B(R)*X 
• Unclustered index on a: cost T(R)*X


• Selection condition: sid > 300 ∧ scity=‘Seattle’ 
– Index I1: B+-tree on sid clustered 
– Index I2: B+-tree on scity unclustered 
• Let’s assume 
– V(Supplier,scity) = 20 
– Max(Supplier, sid) = 1000, Min(Supplier,sid)=1 
– B(Supplier) = 100, T(Supplier) = 1000 
• Cost I1: B(R) * (Max-v)/(Max-Min) = 100*700/999 ≈ 70 
• Cost I2: T(R) * 1/V(Supplier,scity) = 1000/20 = 50 


• For each operator compute
– Estimate cost of executing the operation
– Estimate statistical summary of the output data


Relational Algebra Equivalences 
• Selects, projects, and joins 
– We can commute and combine all three types of operators 
– We just have to be careful that the fields we need are available 
when we apply the operator
– Relatively straightforward. See book 15.3. 
• If you like this topic, more info in optional paper (by 
Chaudhuri), Section 4. 


Plan Enumeration Algorithm 
• Idea: use dynamic programming
• For each subset of {R1, …, Rn}, compute the best plan 
for that subset 
• In increasing order of set cardinality: 
– Step 1: for {R1}, {R2}, …, {Rn} 
– Step 2: for {R1,R2}, {R1,R3}, …, {Rn-1, Rn} 
– … 
– Step n: for {R1, …, Rn} 
• It is a bottom-up strategy 
• A subset of {R1, …, Rn} is also called a subquery 


 Turing awards to database researchers: 
– Charles Bachman 1973 
– Edgar Codd 1981 for inventing relational dbs
– Jim Gray 1998 for inventing transactions 


ACID Properties 
• Atomicity: Either all changes performed by transaction 
occur or none occurs 
• Consistency: A transaction as a whole does not violate 
integrity constraints 
• Isolation: Transactions appear to execute one after the 
other in sequence 
• Durability: If a transaction commits, its changes will survive 
failures


Why is it hard to provide ACID properties? 
• Concurrent operations 
– Isolation problems 
– We saw one example earlier 
• Failures can occur at any time 
– Atomicity and durability problems 
– Next lecture 
• Transaction may need to abort


Types of Problems: Summary 
• Concurrent execution problems
– Write-read conflict: dirty read
• A transaction reads a value written by another transaction that has 
not yet committed 
– Read-write conflict: unrepeatable read
• A transaction reads the value of the same object twice. Another 
transaction modifies that value in between the two reads 
– Write-write conflict: lost update
• Two transactions update the value of the same object. The second 
one to write the value overwrite the first change 



Deadlocks 
• Two or more transactions are waiting for each other to 
complete 
• Deadlock avoidance
– Acquire locks in pre-defined order 
– Acquire all locks at once before starting 
• Deadlock detection
– Timeouts 
– Wait-for graph 
• This is what commercial systems use (they check graph periodically


Degrees of Isolation 
• Isolation level “serializable” (i.e. ACID) 
– Golden standard 
– Requires strict 2PL and predicate locking 
– But often too inefficient 
– Imagine there are only a few update operations and many long 
read operations 
• Weaker isolation levels 
– Sacrifice correctness for efficiency 
– Often used in practice (often default) 
– Sometimes are hard to understand 


Four levels of isolation
– All levels use long-duration exclusive locks
– READ UNCOMMITTED: no read locks 
– READ COMMITTED: short duration read locks 
– REPEATABLE READ: 
• Long duration read locks on individual items 
– SERIALIZABLE: 
• All locks long duration and lock predicates


Lock Granularity 
• Fine granularity locking (e.g., tuples) 
– High concurrency 
– High overhead in managing locks 
• Coarse grain locking (e.g., tables) 
– Many false conflicts 
– Less overhead in managing locks 
• Alternative techniques 
– Hierarchical locking (and intentional locks) [commercial DBMSs] 
– Lock escalation 


The Tree Protocol 
Rules: 
• A lock on a node A may only be acquired if the transaction holds a 
lock on its parent B 
• Nodes can be unlocked in any order (no 2PL necessary) 
• Cannot relock a node for which already released a lock 
• “Crabbing” 
– First lock parent then lock child 
– Keep parent locked only if may need to update it 
– Release lock on parent if child is not full
• The tree protocol is NOT 2PL, yet ensures conflict-serializability !


Optimistic Concurrency Control 
Validation-based technique 
• Phase 1: Read
– Transaction reads from database and writes to a private workspace 
• Phase 2: Validate 
– At commit time, system performs validation 
– Validation checks if transaction could have conflicted with others 
• Each transaction gets a timestamp 
• Check if timestamp order is equivalent to a serial order 
– If there is a potential conflict: abort 
• Phase 3: Write 
– If no conflict, transaction changes are copied into database


Optimistic Concurrency Control 
Timestamp-based technique 
• Each object, O, has read and write timestamps: RTS(O) and WTS(O) 
• Each transaction, T, has a timestamp TS(T) 
• Transaction wants to read object O
– If TS(T) < WTS(O) abort 
– Else read and update RTS(O) to larger of TS(T) or RTS(O) 
• Transaction wants to write object O
– If TS(T) < RTS(O) abort 
– If TS(T) < WTS(O) ignore my write and continue (Thomas Write Rule) 
– Otherwise, write O and update WTS(O) to TS(T) 


Multiversion-based technique 
• Object timestamps: RTS(O) & WTS(O); transaction timestamps TS(T) 
• Transaction can read most recent version that precedes TS(T) 
– When reading object, update RTS(O) to larger of TS(T) or RTS(O) 
• Transaction wants to write object O
– If TS(T) < RTS(O) abort 
– Otherwise, create a new version of O with WTS(O) = TS(T) 
• Common variant (used in commercial systems) 
– To write object O only check for conflicting writes not reads 
– Use locks for writes to avoid aborting in case conflicting transaction aborts


Buffer Manager Policies 
• STEAL or NO-STEAL
– Can an update made by an uncommitted transaction overwrite the most 
recent committed value of a data item on disk? 
• FORCE or NO-FORCE
– Should all updates of a transaction be forced to disk before the 
transaction commits? 
• Easiest for recovery: NO-STEAL/FORCE 
• Highest performance: STEAL/NO-FORCE 


Solution: Use a Log 
• Log: append-only file containing log records
• Enables the use of STEAL and NO-FORCE 
• For every update, commit, or abort operation 
– Write physical, logical, or physiological log record 
– Note: multiple transactions run concurrently, log records are 
interleaved 
• After a system crash, use log to: 
– Redo some transaction that did commit 
– Undo other transactions that didn’t commit 


Write-Ahead Log 
• All log records pertaining to a page are written to disk 
before the page is overwritten on disk 
• All log records for transaction are written to disk before
the transaction is considered committed 
– Why is this faster than FORCE policy? 
• Committed transaction: transactions whose commit log 
record has been written to disk


ARIES Method 
• Write-Ahead Log 
• Three pass algorithm 
– Analysis pass 
• Figure out what was going on at time of crash 
• List of dirty pages and active transactions 
– Redo pass (repeating history principle)
• Redo all operations, even for transactions that will not commit 
• Get back to state at the moment of the crash 
– Undo pass
• Remove effects of all uncommitted transactions 
• Log changes during undo in case of another crash during undo


log sequence number (LSN)

performance metrics for parallel dbms
• Speedup 
– More processors  higher speed
• Scalup
– More processors  can process more data 
– Transaction scaleup vs batch scaleup
• Challenges to speedup and scalup
– Startup cost: cost of starting an operation on many processors 
– Interference: contention for resources between processors 
– Skew: slowest step becomes the bottleneck


Architectures for Parallel Databases 
• Shared memory 
• Shared disk 
• Shared nothing 


Taxonomy for 
Parallel Query Evaluation 
• Inter-query parallelism 
– Each query runs on one processor 
• Inter-operator parallelism 
– A query runs on multiple processors 
– An operator runs on one processor 
• Intra-operator parallelism 
– An operator runs on multiple processors 
13 
We study only intra-operator parallelism: most scalable


Properties of a consensus protocol
• A consensus protocol provides safety if...
- Agreement – All outputs produced have the same value, and
- Validity – The output value equals one of the agents’ inputs
• A consensus protocol provides liveness if...
- Termination – Eventually non-failed agents output a value
• A consensus protocol provides fault tolerance if...
- It can survive the failure of an agent at any point
- Fail-stop protocols handle agent crashes
- Byzantine-fault-tolerant protocols handle arbitrary agent behavior
Theorem (FLP impossibility result)
No deterministic consensus protocol provides all three of safety,
liveness, and fault tolerance in an asynchronous system.



Hashing-Sorting solves “all” known data scale problems :=)


================================================================


# mysql
mysql queris and optimization
99+1 Tips to MySQL Tuning and Optimization
 

 

MySQL is a powerful open-source database.  With more and more database driven applications, people have been pushing MySQL to its limits.  Here are 101 tips for tuning and optimizing your MySQL install.  Some tips are specific to the environment they are installed on, but the concepts are universal.   I have divided them up into several categories to help you with getting the most out of MySQL:
 

transaction monitoring

MySQL Server Hardware and OS Tuning:
1. Have enough physical memory to load your entire InnoDB file into memory – InnoDB is much faster when the file can be accessed in memory rather than from disk.
2. Avoid Swap at all costs – swapping is reading from disk, its slow.
3. Use Battery-Backed RAM.
4. Use an advanced RAID – preferably RAID10 or higher.
5. Avoid RAID5 – the checksum needed to ensure integrity is costly.
6. Separate your OS and data partitions, not just logically, but physically – costly OS writes and reads will impact your database performance.
7. Put your mysql temp space and replication logs on a separate partition than your data – background writes will impact your database when it goes to write/read from disk.
8. More disks equals more speed.
9. Faster disks are better.
10. Use SAS over SATA.
11. Smaller disks are faster than larger disks, especially in RAID configs.
12. Use Battery-Backed Cache RAID controllers.
13. Avoid software raids.
14. Consider using Solid State IO Cards (not disk drives) for your data partition – these cards can sustain over 2GB/s writes for almost any amount of data.
15. On Linux set your swappiness value to 0 – no reason to cache files on a database server, this is more of a web server or desktop advantage.
16. Mount filesystem with noatime and nodirtime if available – no reason to update database file modification times for access.
17. Use XFS filesystem – a faster, smaller filesystem than ext3 and has more options for journaling, also ext3 has been shown to have double buffering issues with MySQL.
18. Tune your XFS filesystem log and buffer variables – for maximum performance benchmark.
19. On Linux systems, use NOOP or DEADLINE IO scheduler – the CFQ and ANTICIPATORY scheduler have been shown to be slow vs NOOP and DEADLINE scheduler.
20. Use a 64-bit OS – more memory addressable and usable to MySQL.
21. Remove unused packages and daemons from servers – less resource stealing.
22. Put your host that use MySQL and your MySQL host in a hosts file – no dns lookups.
23. Never force kill a MySQL process – you will corrupt your database and be running for the backups.
24. Dedicate your server to MySQL – background processes and other services can steal from the db cpu time.

MySQL Configuration:
25. Use innodb_flush_method=O_DIRECT to avoid a double buffer when writing.

26. Avoid O_DIRECT and EXT3 filesystem – you will serialize all your writes.

27. Allocate enough innodb_buffer_pool_size to load your entire InnoDB file into memory – less reads from disk.

28. Do not make innodb_log_file_size too big, with faster and more disks – flushing more often is good and lowers the recovery time during crashes.

29. Do not mix innodb_thread_concurrency and thread_concurrency variables – these two values are not compatible.

30. Allocate a minimal amount for max_connections – too many connections can use up your RAM and lock up your MySQL server.

31. Keep thread_cache at a relatively high number, about 16 – to prevent slowness when opening connections.

32. Use  skip-name-resolve – to remove dns lookups.

33. Use query cache if your queries are repetitive and your data does not change often – however using query cache on data that changes often will give you a performance hit.

34. Increase temp_table_size – to prevent disk writes.

35. Increase max_heap_table_size – to prevent disk writes.

36. Do not set your sort_buffer_size too high – this is per connection and can use up memory fast.

37. Monitor key_read_requests and key_reads to determine your key_buffer size – the key read requests should be higher than your key_reads, otherwise you are not efficiently using your key_buffer.

38. Set innodb_flush_log_at_trx_commit = 0 will improve performance, but leaving it to default (1), you will ensure data integrity, you will also ensure replication is not lagging

39. Have a test environment where you can test your configs and restart often, without affecting production.

MySQL Schema Optimization:

40. Keep your database trim.

41. Archive old data – to remove excessive row returns or searches on queries.

42. Put indexes on your data.

43. Do not overuse indexes, compare with your queries.

44. Compress text and blob data types – to save space and reduce number of disk reads.

45. UTF 8 and UTF16 is slower than latin1.


46. Use Triggers sparingly.

47. Keep redundant data to a minimum – do not duplicate data unnecessarily.

48. Use linking tables rather than extending rows.

49. Pay attention to your data types, use the smallest one possible for your real 
data.

50. Separate blob/text data from other data if other data is often used for queries when blob/text are not.

51. Check and optimize tables often.

52. Rewrite InnoDB tables often to optimize.

53. Sometimes, it is faster to drop indexes when adding columns and then add indexes back.

54. Use different storage engines for different needs.

55. Use ARCHIVE storage engine for Logging tables or Auditing tables – this is much more efficient for writes.

56. Store session data in memcache rather than MySQL – memcache allows for auto-expiring values and prevents you from having to create costly reads and writes to MySQL for temporal data.

57. Use VARCHAR instead CHAR when storing variable length strings – to save space since CHAR is fixed length and VARCHAR is not (utf8 is not affected by this).

58. Make schema changes incrementally – a small change can have drastic effects.

59. Test all schema changes in a development environment that mirrors production.

60. Do NOT arbitrarily change values in your config file, it can have disastrous affects.

61. Sometimes less is more in MySQL configs.

62. When in doubt use a generic MySQL config file.
MySQL metrics widget
Query Optimization:

63. Use the slow query log to find slow queries.

64. Use EXPLAIN to determine queries are functioning appropriately.

65. Test your queries often to see if they are performing optimally – performance 
will change over time.

66. Avoid count(*) on entire tables, it can lock the entire table.

67. Make queries uniform so subsequent similar queries will use query cache.

68. Use GROUP BY instead of DISTINCT when appropriate.

69. Use indexed columns in WHERE, GROUP BY, and ORDER BY clauses.

70. Keep indexes simple, do not reuse a column in multiple indexes.

71. Sometimes MySQL chooses the wrong index, use USE INDEX for this case

72. Check for issues using SQL_MODE=STRICT.

73. Use a LIMIT on UNION instead of OR for less than 5 indexed fields.

74. Use INSERT ON DUPLICATE KEY or INSERT IGNORE instead of UPDATE to avoid the 
SELECT prior to update.

75. Use a indexed field and ORDER BY instead of MAX.

76. Avoid using ORDER BY RAND().

77. LIMIT M,N can actually slow down queries in certain circumstances, use sparingly.

78. Use UNION instead of sub-queries in WHERE clauses.

79. For UPDATES, use SHARE MODE to prevent exclusive locks.

80. On restarts of MySQL, remember to warm your database, to ensure that your data is 
in memory and queries are fast.

81. Use DROP TABLE then CREATE TABLE instead of DELETE FROM to remove all data from a table.

82. Minimize the data in your query to only the data you need, using * is overkill 
most of the time.

83. Consider persistent connections instead of multiple connections to reduce overhead.

84. Benchmark queries, including using load on the server, sometimes a simple query can have affects on other queries.

85. When load increases on your server, use SHOW PROCESSLIST to view slow/problematic queries.

86. Test all suspect queries in a development environment where you have mirrored production data.
MySQL Backup Procedures:

87. Backup from secondary replicated server.

88. Stop replication during backups to prevent inconsistencies on data dependencies and foreign constraints.

89. Stop MySQL altogether and take a backup of the database files.

90. Backup binary logs at same time as dumpfile if MySQL dump used – to make sure replication does not break.

91. Do not trust an LVM snapshot for backups – this could create data inconsistencies that will give you issues in the future.

92. Make dumps per table for easier single table recovery – if data is isolated from other tables.

93. Use –opt when using mysqldump.

94. Check and Optimize tables before a backup.

95. When importing temporarily disable foreign constraints for a faster import.

96. When importing temporarily disable unique checks for a faster import.

97. Calculate size of database/tables data and indexes after each backup to monitor growth.

98. Monitor slave replication for errors and delay with a cron script.

99. Perform Backups regularly.

100. Test your backups regularly.



References:

Cross join:
SELECT *
FROM EMPLOYEE, COMPENSATION ;
Or
SELECT *
FROM EMPLOYEE CROSS JOIN COMPENSATION ;

STRAIGHT_JOIN:
SELECT table112.id,table112.bval1,table112.bval2,
table111.id,table111.aval1
FROM table112
STRAIGHT_JOIN table111;
 Or
It behaves like a inner join if given any condition.

Natural Join:
SELECT E.*, C.Salary, C.Bonus
 FROM EMPLOYEE E NATURAL JOIN COMPENSATION C ;

Or
SELECT E.*, C.Salary, C.Bonus
 FROM EMPLOYEE E, COMPENSATION C
 WHERE E.EmpID = C.EmpID ;


Condition Join:
Example: using ON aggretated function
SELECT *
 FROM NATIONAL JOIN AMERICAN
 ON NATIONAL.CompleteGames = AMERICAN.CompleteGames ;

SQL Predection:

ALL
BETWEEN
DISTINCT
EXISTS
IN
LIKE
MATCH
NOT IN
NOT LIKE
NULL
OVERLAPS
SOME, ANY
UNIQUE



Normalities:
First Normal Form (1NF):
Table must be two-dimensional, with rows and columns.
Each row contains data that pertains to one thing or one portion of a thing.
Each column contains data for a single attribute of the thing being described.
Each cell (intersection of row and column) of the table must be single-valued.
All entries in a column must be of the same kind.
Each column must have a unique name.
No two rows may be identical.
The order of the columns and of the rows does not matter.
Second Normal Form (2NF):
Table must be in first normal form (1NF).
All non-key attributes (columns) must be dependent on the entire key.
Third Normal Form (3NF):
Table must be in second normal form (2NF).
Table has no transitive dependencies.
Domain-Key Normal Form (DK/NF):
Every constraint on the table is a logical consequence of the definition of keys and domains.

SET FUNCTIONS & AGGREGATE FUNCTIONS:

COUNT
Returns the number of rows in the specified table
MAX
Returns the maximum value that occurs in the specified able
MIN
Returns the minimum value that occurs in the specified table
SUM
Adds up the values in a specified column
AVG
Returns the average of all the values in the specified column


TRIGONOMETRIC AND LOGARITHMIC FUNCTIONS

sin, cos, tan, asin, acos, atan, sinh, cosh, tanh, log(<base>, <value>), log10(<value>). ln( <value>)

JSON CONSTRUCTOR FUNCTIONS
JSON_OBJECT
JSON_ARRAY
JSON_OBJECTAGG
JSON_ARRAYAGG
JSON QUERY FUNCTIONS
JSON_EXISTS
JSON_VALUE
JSON_QUERY
JSON_TABLE
DATA TYPES:
Here’s a list of all the formal data types that ISO/IEC standard SQL recognizes. In addition to these, you may define additional data types that are derived from these.
Exact Numerics:
INTEGER
SMALLINT
BIGINT
NUMERIC
DECIMAL
Approximate Numerics:
REAL
DOUBLE PRECISION
FLOAT
DECFLOAT
Binary Strings:
BINARY
BINARY VARYING
BINARY LARGE OBJECT
Boolean:
BOOLEAN
Character Strings:
CHARACTER
CHARACTER VARYING (VARCHAR)
CHARACTER LARGE OBJECT
NATIONAL CHARACTER
NATIONAL CHARACTER VARYING
NATIONAL CHARACTER LARGE OBJECT
Datetimes:
DATE
TIME WITHOUT TIMEZONE
TIMESTAMP WITHOUT TIMEZONE
TIME WITH TIMEZONE
TIMESTAMP WITH TIMEZONE
Intervals:
INTERVAL DAY
INTERVAL YEAR
Collection Types:
ARRAY
MULTISET
Other Types:
ROW
XML


DBMS RAMKRISHNA:
Levels of Abstraction in a DBMS:
Conceptual
Physical
External
data definition language (DDL) is used to define the external and conceptual schemas
Physical schema specifies additional storage details
External schemas, which usually are also in terms of the data model of the DBMS, allow data access to be customized (and authorized) at the level of individual users or groups of users
DATABASE DESIGN AND ER DIAGRAMS:
 Requirements Analysis
Conceptual Database Design
Logical Database Design
Schema Refinement
Physical Database Design | performance
Application and Security Design

ENTITIES, ATTRIBUTES, AND ENTITY SETS:
RELTIONSHIPS AND RELATIONSHIP SETS:
Key Constraints
Key Constraints for Ternary Relationships
Weak Entities
Class Hierarchies
Aggregation
Entity versus Attribute
THE RELATIONAL MODEL:
CASCADE:
The CASCADE option ensures that information about an employee's policy and dependents is deleted if the corresponding Employees tuple is deleted. The Relational 1"1,,1oriel 3.5.6 T
FOREIGN KEY (ssn) REFERENCES Employees ON DELETE CASCADE )
INTRODUCTION TO VIEWS:
A view is a table whose rows are not explicitly stored in the database but are computed as needed from a view definition
Views, Data Independence, Security
RELATIONAL ALGEBRA AND CALCULUS:
SET OPERATIONS:
Union
Intersection
Difference
Cross product
Division (oppo of Joins)
Distinct
RELATIONAL CALCULUS:
The variant of the calculus we present in detail is called the tuple relational calculus (TRC)
Select * from table where(# relational calculus)

SQL: QUERIES, CONSTRAINTS, TRIGGERS:
 Triggers and Advanced Integrity Constraints:
Correlated Nested Queries:
Pind the names of sailors who have reserved boat nv,mber 103. 
SELECT FROM WHERE S.sname Sailors S EXISTS ( SELECT * FROM Reserves R WHERE R.bid = 103 AND R.sid = S.sid )
Set-Comparison Operators:
Find sailors whose rating is better than some sailor called Horatio. 
SELECT S.sid FROM Sailors S WHERE S.rating > ANY ( SELECT FROM WHERE S2.rating Sailors S2 S2.sname = 'Horatio' )
The GROUP BY and HAVING Clauses
STORAGE AND INDEXING:
INDEX DATA STRUCTURES:
One way to organize data entries is to hash data entries on the sea.rch key
Tree-Based Indexing:
 An alternative to hash-based indexing is to organize records using a treelike data structure. The data entries are arranged in sorted order by search key value, and a hierarchical search data structure is maintained that directs searches to the correct page of data entries.
COMPARISON OF FILE ORGANIZATIONS:


Exporting Data with the SELECT ... INTO OUTFILE Statement

SELECT * FROM passwd INTO OUTFILE '/tmp/tutorials.txt'
   -> FIELDS TERMINATED BY ',' ENCLOSED BY '"'
   -> LINES TERMINATED BY '\r\n';

mysqldump -u root -p database_name table_name > dump.txt
password *****
 IMPORT:
LOAD DATA LOCAL INFILE 'dump.txt' INTO TABLE mytbl;
mysqlimport -u root -p --local --fields-terminated-by = ":" \
   --lines-terminated-by = "\r\n"  database_name dump.txt
password *****

The SQL IN Operator
The IN operator allows you to specify multiple values in a WHERE clause.
The IN operator is a shorthand for multiple OR conditions.

Case:
SELECT OrderID, Quantity,
CASE
    WHEN Quantity > 30 THEN "The quantity is greater than 30"
    WHEN Quantity = 30 THEN "The quantity is 30"
    ELSE "The quantity is under 30"
END
FROM OrderDetails;

The MySQL IFNULL() function lets you return an alternative value if an expression is NULL:
SELECT ProductName, UnitPrice * (UnitsInStock + IFNULL(UnitsOnOrder, 0))
FROM Products

Stored Procedure Syntax
CREATE PROCEDURE procedure_name
AS
sql_statement
GO;

Execute a Stored Procedure
EXEC procedure_name;
With param:
CREATE PROCEDURE SelectAllCustomers @City nvarchar(30)
AS
SELECT * FROM Customers WHERE City = @City
GO;
EXEC SelectAllCustomers City = "London";

Introduction to MySQL ENUM data type
In MySQL, an ENUM is a string object whose value is chosen from a list of permitted values defined at the time of column creation.

CREATE TABLE table_name (
    ...
    col ENUM ('value1','value2','value3'),
    ...
);

CREATE TABLE tickets (
    id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(255) NOT NULL,
    priority ENUM('Low', 'Medium', 'High') NOT NULL
);
INSERT INTO tickets(title, priority)
VALUES('Scan virus for computer A', 'High');

Cascade 2:

CREATE TABLE rooms (
    room_no INT PRIMARY KEY AUTO_INCREMENT,
    room_name VARCHAR(255) NOT NULL,
    building_no INT NOT NULL,
    FOREIGN KEY (building_no)
        REFERENCES buildings (building_no)
        ON DELETE CASCADE
);


Mysql json:
CREATE TABLE table_name (
    ...
    json_column_name JSON,  
    ...
);

Triggers:


2
3
4
5
6
7
8
DELIMITER $$
CREATE TRIGGER  trigger_name
[BEFORE|AFTER] [INSERT|UPDATE|DELETE] ON table_name
FOR EACH ROW [FOLLOWS|PRECEDES] existing_trigger_name
BEGIN
…
END$$
DELIMITER ;


DELIMITER $$
 
CREATE TRIGGER before_products_update
   BEFORE UPDATE ON products
   FOR EACH ROW
BEGIN
    INSERT INTO price_logs(product_code,price)
    VALUES(old.productCode,old.msrp);
END$$
 
DELIMITER ;


Node.js         49.6%
Angular         36.9%
React               27.8%
Spring          17.6%
Django          13.0%

Cordova         8.5%            Spark               4.8%
Hadoop          4.7%            TensorFlow          7.8%
Xamarin         7.4%            Torch/PyTorch       1.7%

MySQL
58.6%
SQL Server
41.6%
PostgreSQL
33.3%
MongoDB
26.4%
SQLite
19.7%
Redis
18.5%
Elasticsearch
14.4%
MariaDB
13.5%
Oracle
11.1%
Microsoft Azure (Tables, CosmosDB, SQL, etc)
8.0%
Memcached
5.6%
Google Cloud Storage
5.5%
Amazon DynamoDB
5.3%
Amazon RDS/Aurora
5.2%
Cassandra
3.7%
IBM Db2
2.5%
Neo4j
2.4%
Amazon Redshift
2.2%
Apache Hive
2.2%
Google BigQuery
2.1%
Apache HBase
1.7%

=============================================



Covering Indexes


Slow Query Basics: Optimize Data Access
The most basic reason a query doesn’t perform well is because it’s working with too
much data. Some queries just have to sift through a lot of data, which can’t be helped.
That’s unusual, though; most bad queries can be changed to access less data. We’ve
found it useful to analyze a poorly performing query in two steps:
1. Find out whether your application is retrieving more data than you need. That
usually means it’s accessing too many rows, but it might also be accessing too
many columns.
2. Find out whether the MySQL server is analyzing more rows than it needs.



Here are a few typical mistakes:

Fetching more rows than needed
Fetching all columns from a multitable join
Fetching all columns
Fetching the same data repeatedly


 In MySQL, the simplest
query cost metrics are:
• Response time
• Number of rows examined
• Number of rows returned



If you find that a huge number of rows were examined to produce relatively few rows
in the result, you can try some more sophisticated fixes:
• Use covering indexes, which store data so that the storage engine doesn’t have to
retrieve the complete rows. (We discussed these in Chapter 7.)
• Change the schema. An example is using summary tables (discussed in
Chapter 6).
• Rewrite a complicated query so the MySQL optimizer is able to execute it opti‐
mally. (We discuss this later in this chapter.)



At a high level, replication is a simple three-part process:
1. The source records changes to its data in its binary log as “binary log events.”
2. The replica copies the source’s binary log events to its own local relay log.
3. The replica replays the events in the relay log, applying the changes to its own
data.




Fundamental operations to retrieve
and manipulate tuples in a relation.
→ Based on set algebra.
Each operator takes one or more
relations as its inputs and outputs a
new relation.
→ We can "chain" operators together to create
more complex operations.


Data Manipulation Language (DML)
Data Definition Language (DDL)
Data Control Language (DCL)



group by 
Project tuples into subsets and 
calculate aggregates against
each subset.


Non-aggregated values in SELECT output clause 
must appear in GROUP BY clause.

having 对 group by 进行过滤
Filters results based on aggregation computation.
Like a WHERE clause for a GROUP BY


NESTED QUERIES
Queries containing other queries.
They are often difficult to optimize. 
Inner queries can appear (almost) anywhere in 
query.



ALL→ Must satisfy expression for all rows in the 
sub-query.
ANY→ Must satisfy expression for at least one row 
in the sub-query.
IN→ Equivalent to '=ANY()' .
EXISTS→ At least one row is returned.


SEQUENTIAL VS. RANDOM ACCESS


SYSTEM DESIGN GOALS
Allow the DBMS to manage databases that exceed 
the amount of memory available.
Reading/writing to disk is expensive, so it must be 
managed carefully to avoid large stalls and 
performance degradation.
Random access on disk is usually much slower 
than sequential access, so the DBMS will want to 
maximize sequential access.